{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple RAG (Retrieval-Augmented Generation) Implementation with LangChain 1.0+\n",
    "\n",
    "## Overview\n",
    "This notebook demonstrates a complete RAG pipeline using **LangChain 1.0+ with LCEL** (LangChain Expression Language).\n",
    "\n",
    "### What is RAG?\n",
    "RAG combines retrieval of relevant documents with generation from a Large Language Model (LLM):\n",
    "1. **Retrieval**: Find relevant information from a knowledge base\n",
    "2. **Augmentation**: Add retrieved context to the prompt\n",
    "3. **Generation**: LLM generates answers based on the context\n",
    "\n",
    "### Pipeline Flow:\n",
    "```\n",
    "PDF Documents ‚Üí Load ‚Üí Split into Chunks ‚Üí Create Embeddings ‚Üí Store in Vector DB\n",
    "                                                                         ‚Üì\n",
    "User Query ‚Üí Retrieve Similar Chunks ‚Üí Combine with Query ‚Üí LLM ‚Üí Answer\n",
    "```\n",
    "\n",
    "### Components Used:\n",
    "- **Document Loader**: PyPDFLoader (for PDF processing)\n",
    "- **Text Splitter**: RecursiveCharacterTextSplitter (smart chunking)\n",
    "- **Embeddings**: OpenAI text-embedding-3-small (vector representations)\n",
    "- **Vector Store**: FAISS (fast similarity search)\n",
    "- **LLM**: OpenAI GPT-4-Turbo or GPT-3.5-Turbo\n",
    "- **Chain Builder**: LCEL (LangChain Expression Language)\n",
    "\n",
    "### LangChain 1.0+ Features:\n",
    "- ‚úÖ Modern LCEL syntax with pipe operator `|`\n",
    "- ‚úÖ More readable and composable chains\n",
    "- ‚úÖ Better streaming support\n",
    "- ‚úÖ Type-safe operations\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation & Setup\n",
    "\n",
    "First, install all required packages. Make sure you have Python 3.9+ installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# Uncomment and run ONE of the following options:\n",
    "\n",
    "# Option 1: Install from requirements.txt (RECOMMENDED)\n",
    "# !pip install -r requirements.txt\n",
    "\n",
    "# Option 2: Install all packages individually\n",
    "# !pip install langchain langchain-core langchain-openai langchain-community langchain-text-splitters faiss-cpu pypdf python-dotenv tiktoken jupyter notebook\n",
    "\n",
    "# Option 3: Quick install (if you're having import issues)\n",
    "# !pip install --upgrade langchain langchain-core langchain-openai langchain-community langchain-text-splitters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Required Libraries\n",
    "\n",
    "Import all necessary modules with explanations of what each does."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify Installation\n",
    "\n",
    "If you encounter import errors, run this cell first to check package versions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain langchain-core langchain-openai langchain-community langchain-text-splitters faiss-cpu pypdf python-dotenv tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì langchain: 1.1.0\n",
      "‚úì langchain-core: 1.1.0\n",
      "‚úì langchain-openai: 1.1.0\n",
      "‚úì langchain-community: 0.4.1\n",
      "\n",
      "Python version: 3.13.5 | packaged by Anaconda, Inc. | (main, Jun 12 2025, 16:37:03) [MSC v.1929 64 bit (AMD64)]\n",
      "\n",
      "If any packages are missing, run:\n",
      "pip install langchain langchain-core langchain-openai langchain-community langchain-text-splitters faiss-cpu pypdf python-dotenv tiktoken\n"
     ]
    }
   ],
   "source": [
    "# Check installed package versions\n",
    "import sys\n",
    "from importlib.metadata import version\n",
    "\n",
    "try:\n",
    "    import langchain\n",
    "    print(f\"‚úì langchain: {langchain.__version__}\")\n",
    "except:\n",
    "    print(\"‚úó langchain not installed\")\n",
    "\n",
    "try:\n",
    "    import langchain_core\n",
    "    print(f\"‚úì langchain-core: {langchain_core.__version__}\")\n",
    "except:\n",
    "    print(\"‚úó langchain-core not installed - REQUIRED!\")\n",
    "    print(\"  Run: pip install langchain-core\")\n",
    "\n",
    "try:\n",
    "    import langchain_openai\n",
    "    print(f\"‚úì langchain-openai: {version('langchain-openai')}\")\n",
    "except:\n",
    "    print(\"‚úó langchain-openai not installed\")\n",
    "\n",
    "try:\n",
    "    import langchain_community\n",
    "    print(f\"‚úì langchain-community: {langchain_community.__version__}\")\n",
    "except:\n",
    "    print(\"‚úó langchain-community not installed\")\n",
    "\n",
    "print(f\"\\nPython version: {sys.version}\")\n",
    "print(\"\\nIf any packages are missing, run:\")\n",
    "print(\"pip install langchain langchain-core langchain-openai langchain-community langchain-text-splitters faiss-cpu pypdf python-dotenv tiktoken\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì All imports successful!\n",
      "‚úì Compatible with LangChain 1.0+\n"
     ]
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "from pathlib import Path\n",
    "\n",
    "# Environment variable management - for secure API key handling\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# LangChain Document Loaders - for loading PDF documents\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "# LangChain Text Splitters - for breaking documents into manageable chunks\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# OpenAI Integration - for embeddings and LLM\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "\n",
    "# Vector Store - FAISS for efficient similarity search\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# LangChain Core Components\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "print(\"‚úì All imports successful!\")\n",
    "print(\"‚úì Compatible with LangChain 1.0+\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Environment Configuration\n",
    "\n",
    "### Setting up OpenAI API Key\n",
    "\n",
    "You have two options:\n",
    "1. **Recommended**: Create a `.env` file with `OPENAI_API_KEY=your_key_here`\n",
    "2. **Alternative**: Set it directly in code (not recommended for production)\n",
    "\n",
    "Get your API key from: https://platform.openai.com/api-keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì OpenAI API Key loaded successfully!\n",
      "‚úì Key starts with: sk-svcac...\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Verify API key is loaded\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    print(\"‚ö†Ô∏è  WARNING: OPENAI_API_KEY not found!\")\n",
    "    print(\"Please set it in .env file or uncomment the line below:\")\n",
    "    # os.environ[\"OPENAI_API_KEY\"] = \"your_api_key_here\"\n",
    "else:\n",
    "    api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    print(\"‚úì OpenAI API Key loaded successfully!\")\n",
    "    print(f\"‚úì Key starts with: {api_key[:8]}...\" if api_key else \"‚úì API Key is set but could not be displayed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Document Loading\n",
    "\n",
    "### Loading PDF Documents\n",
    "\n",
    "PyPDFLoader extracts text from PDF files page by page. Each page becomes a separate document with metadata (page number, source file).\n",
    "\n",
    "**How it works:**\n",
    "- Reads PDF files and extracts text content\n",
    "- Preserves page numbers for source tracking\n",
    "- Returns Document objects with `.page_content` and `.metadata`\n",
    "\n",
    "**Note**: Update the `pdf_path` variable to point to your PDF file(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Loaded 15 pages from '.\\pdfs\\attention.pdf'\n",
      "\n",
      "--- First Document Preview ---\n",
      "Content (first 500 chars): Provided proper attribution is provided, Google hereby grants permission to\n",
      "reproduce the tables and figures in this paper solely for use in journalistic or\n",
      "scholarly works.\n",
      "Attention Is All You Need\n",
      "Ashish Vaswani‚àó\n",
      "Google Brain\n",
      "avaswani@google.com\n",
      "Noam Shazeer‚àó\n",
      "Google Brain\n",
      "noam@google.com\n",
      "Niki Parmar‚àó\n",
      "Google Research\n",
      "nikip@google.com\n",
      "Jakob Uszkoreit‚àó\n",
      "Google Research\n",
      "usz@google.com\n",
      "Llion Jones‚àó\n",
      "Google Research\n",
      "llion@google.com\n",
      "Aidan N. Gomez‚àó ‚Ä†\n",
      "University of Toronto\n",
      "aidan@cs.toronto.edu\n",
      "≈Åukasz ...\n",
      "\n",
      "Metadata: {'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '.\\\\pdfs\\\\attention.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1'}\n",
      "\n",
      "Total characters across all pages: 39,587\n"
     ]
    }
   ],
   "source": [
    "# ===== CONFIGURATION: Update this path to your PDF file =====\n",
    "pdf_path = \".\\\\pdfs\\\\attention.pdf\"  # Change this to your PDF file path\n",
    "# =============================================================\n",
    "\n",
    "# Check if file exists\n",
    "if not os.path.exists(pdf_path):\n",
    "    print(f\"‚ö†Ô∏è  ERROR: File '{pdf_path}' not found!\")\n",
    "    print(\"Please update the pdf_path variable with your PDF file location.\")\n",
    "else:\n",
    "    # Initialize the PDF loader\n",
    "    loader = PyPDFLoader(pdf_path)\n",
    "    \n",
    "    # Load all pages from the PDF\n",
    "    # Each page becomes a separate Document object\n",
    "    documents = loader.load()\n",
    "    \n",
    "    # Display information about loaded documents\n",
    "    print(f\"‚úì Loaded {len(documents)} pages from '{pdf_path}'\")\n",
    "    print(f\"\\n--- First Document Preview ---\")\n",
    "    print(f\"Content (first 500 chars): {documents[0].page_content[:500]}...\")\n",
    "    print(f\"\\nMetadata: {documents[0].metadata}\")\n",
    "    print(f\"\\nTotal characters across all pages: {sum(len(doc.page_content) for doc in documents):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Multiple PDFs (Optional)\n",
    "\n",
    "If you have multiple PDF files, you can load them all at once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 PDF files\n",
      "  ‚úì Loaded 15 pages from attention.pdf\n",
      "  ‚úì Loaded 15 pages from attention.pdf\n",
      "  ‚úì Loaded 19 pages from rag.pdf\n",
      "  ‚úì Loaded 19 pages from rag.pdf\n",
      "  ‚úì Loaded 21 pages from ragsurvey.pdf\n",
      "\n",
      "Total pages loaded: 55\n",
      "  ‚úì Loaded 21 pages from ragsurvey.pdf\n",
      "\n",
      "Total pages loaded: 55\n"
     ]
    }
   ],
   "source": [
    "# Example: Loading multiple PDFs from a directory\n",
    "# Uncomment and modify if you want to load multiple files\n",
    "\n",
    "pdf_directory = \"./pdfs\"  # Directory containing your PDFs\n",
    "all_documents = []\n",
    "\n",
    "if os.path.exists(pdf_directory):\n",
    "    pdf_files = list(Path(pdf_directory).glob(\"*.pdf\"))\n",
    "    print(f\"Found {len(pdf_files)} PDF files\")\n",
    "    \n",
    "    for pdf_file in pdf_files:\n",
    "        loader = PyPDFLoader(str(pdf_file))\n",
    "        docs = loader.load()\n",
    "        all_documents.extend(docs)\n",
    "        print(f\"  ‚úì Loaded {len(docs)} pages from {pdf_file.name}\")\n",
    "    \n",
    "    print(f\"\\nTotal pages loaded: {len(all_documents)}\")\n",
    "    documents = all_documents  # Use this for the rest of the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Text Splitting\n",
    "\n",
    "### Why Split Documents?\n",
    "- LLMs have token limits (e.g., 4K, 8K, 128K tokens)\n",
    "- Smaller chunks = more precise retrieval\n",
    "- Balance: chunks must be large enough to contain meaningful context but small enough to be specific\n",
    "\n",
    "### RecursiveCharacterTextSplitter\n",
    "This splitter tries to keep related text together by recursively splitting on:\n",
    "1. Paragraphs (`\\n\\n`)\n",
    "2. Lines (`\\n`)\n",
    "3. Sentences (`. `)\n",
    "4. Words (` `)\n",
    "5. Characters (as last resort)\n",
    "\n",
    "**Parameters:**\n",
    "- `chunk_size=1024`: Target size for each chunk (in characters)\n",
    "- `chunk_overlap=128`: Overlap between chunks to maintain context continuity\n",
    "- Overlap prevents important information from being split across chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Split 55 documents into 267 chunks\n",
      "\n",
      "Average chunk size: 898 characters\n",
      "\n",
      "--- Chunk Examples ---\n",
      "\n",
      "Chunk 1 (length: 986 chars):\n",
      "Provided proper attribution is provided, Google hereby grants permission to\n",
      "reproduce the tables and figures in this paper solely for use in journalistic or\n",
      "scholarly works.\n",
      "Attention Is All You Need\n",
      "...\n",
      "Metadata: {'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'pdfs\\\\attention.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1'}\n",
      "\n",
      "Chunk 2 (length: 944 chars):\n",
      "based solely on attention mechanisms, dispensing with recurrence and convolutions\n",
      "entirely. Experiments on two machine translation tasks show these models to\n",
      "be superior in quality while being more pa...\n",
      "Metadata: {'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'pdfs\\\\attention.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1'}\n",
      "\n",
      "Chunk 3 (length: 986 chars):\n",
      "‚àóEqual contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\n",
      "the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Tra...\n",
      "Metadata: {'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'pdfs\\\\attention.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1'}\n"
     ]
    }
   ],
   "source": [
    "# Initialize the text splitter with recommended settings\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1024,        # Maximum characters per chunk (roughly 200-250 tokens)\n",
    "    chunk_overlap=128,      # Characters overlap between chunks (maintains context)\n",
    "    length_function=len,    # Function to measure chunk length\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]  # Try to split on paragraphs first, then lines, etc.\n",
    ")\n",
    "\n",
    "# Split the documents into chunks\n",
    "# This creates smaller, manageable pieces while preserving semantic meaning\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "# Display splitting results\n",
    "print(f\"‚úì Split {len(documents)} documents into {len(chunks)} chunks\")\n",
    "print(f\"\\nAverage chunk size: {sum(len(chunk.page_content) for chunk in chunks) / len(chunks):.0f} characters\")\n",
    "\n",
    "# Preview a few chunks\n",
    "print(f\"\\n--- Chunk Examples ---\")\n",
    "for i, chunk in enumerate(chunks[:3]):\n",
    "    print(f\"\\nChunk {i+1} (length: {len(chunk.page_content)} chars):\")\n",
    "    print(f\"{chunk.page_content[:200]}...\")\n",
    "    print(f\"Metadata: {chunk.metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Creating Embeddings\n",
    "\n",
    "### What are Embeddings?\n",
    "Embeddings are vector representations of text that capture semantic meaning. Similar texts have similar vectors.\n",
    "\n",
    "**Example**: \n",
    "- \"dog\" and \"puppy\" ‚Üí similar vectors (close in vector space)\n",
    "- \"dog\" and \"spaceship\" ‚Üí different vectors (far apart)\n",
    "\n",
    "### OpenAI text-embedding-3-small\n",
    "- **Dimensions**: 1536 (each text becomes a 1536-dimensional vector)\n",
    "- **Cost**: $0.00002 per 1,000 tokens (very affordable)\n",
    "- **Performance**: 62.3% on MTEB benchmark\n",
    "- **Speed**: Fast and efficient\n",
    "\n",
    "**Alternative**: `text-embedding-3-large` for higher quality (64.6% MTEB) at higher cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Embeddings model initialized: text-embedding-3-small\n",
      "‚úì Embedding dimension: 1536\n",
      "‚úì Sample embedding (first 10 values): [0.020370882004499435, -0.0031641265377402306, -0.0005454652709886432, 0.0045827641151845455, -0.015004359185695648, -0.034060992300510406, 0.0176328606903553, 0.01959054544568062, 0.0013125392142683268, 0.00596546521410346]\n",
      "\n",
      "‚ÑπÔ∏è  Each chunk will be converted to a 1536-dimensional vector for similarity search\n"
     ]
    }
   ],
   "source": [
    "# Initialize OpenAI Embeddings\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-small\",  # Latest, cost-effective embedding model\n",
    "    # dimensions=1536\n",
    "    # Alternative: \"text-embedding-3-large\" for better quality\n",
    ")\n",
    "\n",
    "# Test the embeddings with a sample text\n",
    "sample_text = \"This is a test sentence to demonstrate embeddings.\"\n",
    "sample_embedding = embeddings.embed_query(sample_text)\n",
    "\n",
    "print(f\"‚úì Embeddings model initialized: text-embedding-3-small\")\n",
    "print(f\"‚úì Embedding dimension: {len(sample_embedding)}\")\n",
    "print(f\"‚úì Sample embedding (first 10 values): {sample_embedding[:10]}\")\n",
    "print(f\"\\n‚ÑπÔ∏è  Each chunk will be converted to a {len(sample_embedding)}-dimensional vector for similarity search\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google Gemini Embeddings (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Embeddings model initialized: gemini-embedding-001\n",
      "‚úì Embedding dimension: 1536\n",
      "‚úì Sample embedding (first 10 values): [-0.029574524611234665, 0.022307422012090683, 0.006308525335043669, -0.08331369608640671, -0.0002317591424798593, -0.003521788865327835, 0.006556599400937557, 0.011158722452819347, 0.008888664655387402, -0.009683165699243546]\n",
      "‚úì Each chunk will be converted to a 1536-dimensional vector for similarity search\n"
     ]
    }
   ],
   "source": [
    "# Below is the code for Gemini Embeddings:\n",
    "\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/gemini-embedding-001\")\n",
    "\n",
    "# # Test the embeddings with a sample text\n",
    "sample_text = \"This is a test sentence to demonstrate embeddings.\"\n",
    "\n",
    "# Control your dimensionality by the parameter herein below.. \n",
    "\n",
    "sample_embedding = embeddings.embed_query(sample_text,output_dimensionality=1536)\n",
    "\n",
    "print(f\"‚úì Embeddings model initialized: gemini-embedding-001\")\n",
    "print(f\"‚úì Embedding dimension: {len(sample_embedding)}\")\n",
    "print(f\"‚úì Sample embedding (first 10 values): {sample_embedding[:10]}\")\n",
    "print(f\"‚úì Each chunk will be converted to a {len(sample_embedding)}-dimensional vector for similarity search\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Creating Vector Store (FAISS)\n",
    "\n",
    "### What is a Vector Store?\n",
    "A vector store (or vector database) stores embeddings and enables fast similarity search.\n",
    "\n",
    "### FAISS (Facebook AI Similarity Search)\n",
    "- **Fast**: Optimized for billion-scale vector search\n",
    "- **Local**: Runs on your machine, no cloud dependency\n",
    "- **Efficient**: Uses advanced indexing algorithms\n",
    "\n",
    "### How Similarity Search Works:\n",
    "1. Convert query to embedding vector\n",
    "2. Find vectors in the database most similar to query vector (using cosine similarity or Euclidean distance)\n",
    "3. Return the corresponding text chunks\n",
    "\n",
    "**This cell will:**\n",
    "1. Convert all chunks to embeddings (may take a minute for large documents)\n",
    "2. Build a FAISS index\n",
    "3. Save to disk for future use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FAISS-CPU Version Compatibility Issue\n",
    "\n",
    "If you encounter issues with `faiss-cpu` installation, try:\n",
    "\n",
    "```bash\n",
    "uv pip uninstall faiss-cpu\n",
    "uv pip install faiss-cpu==1.12.0\n",
    "```\n",
    "\n",
    "Or for conda users:\n",
    "```bash\n",
    "conda install -c conda-forge faiss-cpu==1.12.0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: faiss-cpu\n",
      "Version: 1.12.0\n",
      "Location: d:\\AiCode\\LcRAGBtCmp\\simple-rag-langchain\\lcrag_venv\\Lib\\site-packages\n",
      "Requires: numpy, packaging\n",
      "Required-by:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.13.5 environment at: lcrag_venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv pip show faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Billing Set-up to the following Account - Capital One Watch !\n",
      "KrshNaik-AgentiAI-Gogle-Gemini  \tgen-lang-client-0899462979\n",
      "Creating FAISS index from 267 chunks...\n",
      "This may take a minute depending on the number of chunks...\n",
      "‚úì FAISS vector store created successfully!\n",
      "‚úì Indexed 267 document chunks\n",
      "‚úì Vector store saved to './faiss_index'\n",
      "\n",
      "‚ÑπÔ∏è  You can reload this index later using: FAISS.load_local('./faiss_index', embeddings)\n",
      "‚úì FAISS vector store created successfully!\n",
      "‚úì Indexed 267 document chunks\n",
      "‚úì Vector store saved to './faiss_index'\n",
      "\n",
      "‚ÑπÔ∏è  You can reload this index later using: FAISS.load_local('./faiss_index', embeddings)\n"
     ]
    }
   ],
   "source": [
    "# Create FAISS vector store from document chunks\n",
    "# This step converts each chunk to an embedding and stores it\n",
    "print(f\"Billing Set-up to the following Account - Capital One Watch !\")\n",
    "print(f\"KrshNaik-AgentiAI-Gogle-Gemini  \tgen-lang-client-0899462979\")\n",
    "print(f\"Creating FAISS index from {len(chunks)} chunks...\")\n",
    "print(\"This may take a minute depending on the number of chunks...\")\n",
    "\n",
    "#Billing Set-up to the following Account - Capital One Watch !\n",
    "#KrshNaik-AgentiAI-Gogle-Gemini  \tgen-lang-client-0899462979\n",
    "#Creating FAISS index from 267 chunks...\n",
    "#This may take a minute depending on the number of chunks...\n",
    "#‚úì FAISS vector store created successfully!\n",
    "#‚úì Indexed 267 document chunks\n",
    "#‚úì Vector store saved to './faiss_index'\n",
    "\n",
    "#‚ÑπÔ∏è  You can reload this index later using: FAISS.load_local('./faiss_index', embeddings)\n",
    "\n",
    "vectorstore = FAISS.from_documents(\n",
    "    documents=chunks,      # Our split document chunks\n",
    "    embedding=embeddings   # OpenAI embedding model\n",
    ")\n",
    "\n",
    "print(f\"‚úì FAISS vector store created successfully!\")\n",
    "print(f\"‚úì Indexed {len(chunks)} document chunks\")\n",
    "\n",
    "# Save the vector store to disk for later use\n",
    "# This allows you to reload the index without re-processing documents\n",
    "vectorstore_path = \"./faiss_index\"\n",
    "vectorstore.save_local(vectorstore_path)\n",
    "print(f\"‚úì Vector store saved to '{vectorstore_path}'\")\n",
    "print(f\"\\n‚ÑπÔ∏è  You can reload this index later using: FAISS.load_local('{vectorstore_path}', embeddings)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChromaDB Vector Store (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating ChromaDB from 267 chunks...\n",
      "‚úì ChromaDB vector store created!\n"
     ]
    }
   ],
   "source": [
    "#ChromaDB has better Python 3.13 support. Replace cells 21-24 with:\n",
    "\n",
    "# Instead of FAISS, use ChromaDB\n",
    "#Billing Set-up to the following Account - Capital One Watch !\n",
    "#KrshNaik-AgentiAI-Gogle-Gemini  \tgen-lang-client-0899462979\n",
    "\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# # Create ChromaDB vector store\n",
    "print(f\"Creating ChromaDB from {len(chunks)} chunks...\")\n",
    "#vectorstore = Chroma.from_documents(\n",
    "#     documents=chunks,\n",
    "#     embedding=embeddings,\n",
    "#     persist_directory=\"./chroma_db\"\n",
    "#)\n",
    "print(\"‚úì ChromaDB vector store created!\")\n",
    "\n",
    "#Creating ChromaDB from 267 chunks...\n",
    "#‚úì ChromaDB vector store created!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading a Saved Vector Store (Optional)\n",
    "\n",
    "If you've already created a vector store, you can load it instead of recreating:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Loaded existing vector store from './faiss_index'\n"
     ]
    }
   ],
   "source": [
    "#Uncomment to load an existing vector store instead of creating a new one\n",
    "vectorstore_path = \"./faiss_index\"\n",
    "vectorstore = FAISS.load_local(\n",
    "    vectorstore_path, \n",
    "    embeddings,\n",
    "    allow_dangerous_deserialization=True  # Required for loading pickled data\n",
    " )\n",
    "print(f\"‚úì Loaded existing vector store from '{vectorstore_path}'\")\n",
    "\n",
    "# vectorstore_path = \"./faiss_index\" --- IGNORE ---\n",
    "# vectorstore = FAISS.load_local( --- IGNORE ---\n",
    "#     vectorstore_path,  --- IGNORE ---\n",
    "#     embeddings, --- IGNORE ---\n",
    "#     allow_dangerous_deserialization=True  # Required for loading pickled data --- IGNORE ---\n",
    "#  ) --- IGNORE ---\n",
    "# print(f\"‚úì Loaded existing vector store from '{vectorstore_path}'\") --- IGNORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Retriever configured successfully\n",
      "  - Search type: similarity\n",
      "  - Number of documents to retrieve (k): 4\n",
      "\n",
      "--- Retriever Test ---\n",
      "Query: 'What is the main topic of this document?'\n",
      "Retrieved 4 documents:\n",
      "\n",
      "Document 1:\n",
      "  Content preview: 1\n",
      "Retrieval-Augmented Generation for Large\n",
      "Language Models: A Survey\n",
      "Yunfan Gaoa, Yun Xiongb, Xinyu Gao b, Kangxiang Jia b, Jinliu Pan b, Yuxi Bic, Yi...\n",
      "  Metadata: {'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'pdfs\\\\ragsurvey.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1'}\n",
      "\n",
      "Document 2:\n",
      "  Content preview: for continuous knowledge updates and integration of domain-\n",
      "specific information. RAG synergistically merges LLMs‚Äô intrin-\n",
      "sic knowledge with the vast...\n",
      "  Metadata: {'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'pdfs\\\\ragsurvey.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1'}\n",
      "\n",
      "Document 3:\n",
      "  Content preview: Jonathan Berant. Coarse-to-Ô¨Åne question answering for long documents. In Proceedings of the\n",
      "55th Annual Meeting of the Association for Computational L...\n",
      "  Metadata: {'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'author': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'pdfs\\\\rag.pdf', 'total_pages': 19, 'page': 9, 'page_label': '10'}\n",
      "\n",
      "Document 4:\n",
      "  Content preview: ‚Ä¢ In this survey, we present a thorough and systematic\n",
      "review of the state-of-the-art RAG methods, delineating\n",
      "its evolution through paradigms includi...\n",
      "  Metadata: {'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'pdfs\\\\ragsurvey.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1'}\n"
     ]
    }
   ],
   "source": [
    "# Create a retriever from the vector store\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",    # Use cosine similarity for search\n",
    "    search_kwargs={\"k\": 4}        # Retrieve top 4 most relevant chunks\n",
    ")\n",
    "\n",
    "print(\"‚úì Retriever configured successfully\")\n",
    "print(f\"  - Search type: similarity\")\n",
    "print(f\"  - Number of documents to retrieve (k): 4\")\n",
    "\n",
    "# Test the retriever with a sample query\n",
    "# Note: In LangChain 1.0+, use .invoke() instead of .get_relevant_documents()\n",
    "test_query = \"What is the main topic of this document?\"\n",
    "retrieved_docs = retriever.invoke(test_query)  # LangChain 1.0+ method\n",
    "\n",
    "print(f\"\\n--- Retriever Test ---\")\n",
    "print(f\"Query: '{test_query}'\")\n",
    "print(f\"Retrieved {len(retrieved_docs)} documents:\")\n",
    "\n",
    "for i, doc in enumerate(retrieved_docs):\n",
    "    print(f\"\\nDocument {i+1}:\")\n",
    "    print(f\"  Content preview: {doc.page_content[:150]}...\")\n",
    "    print(f\"  Metadata: {doc.metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Configuring the Language Model (LLM)\n",
    "\n",
    "### LLM Selection\n",
    "The LLM generates the final answer based on retrieved context.\n",
    "\n",
    "### Available Models:\n",
    "1. **gpt-4-turbo-2025-04-09**: Most capable, best quality, slower, more expensive\n",
    "2. **gpt-4o**: Fast GPT-4 level performance, good balance\n",
    "3. **gpt-3.5-turbo**: Fast and cheap, good for simpler queries\n",
    "\n",
    "### Temperature:\n",
    "- **0**: Deterministic, focused answers (recommended for factual Q&A)\n",
    "- **0.7**: More creative, varied responses\n",
    "- **1.0**: Most creative, less predictable\n",
    "\n",
    "### Max Tokens:\n",
    "Controls the maximum length of the generated response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì LLM configured successfully\n",
      "  - Model: gpt-4o-mini\n",
      "  - Temperature: 0 (deterministic)\n",
      "  - Max tokens: 2000\n",
      "\n",
      "LLM Test Response: Hello, I am ready to answer questions!\n",
      "\n",
      "LLM Test Response: Hello, I am ready to answer questions!\n"
     ]
    }
   ],
   "source": [
    "# Initialize the ChatOpenAI model\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "      model=\"gpt-4o-mini\",  # \"gpt-4-turbo-2024-04-09\",  # Choose your model\n",
    "      # Alternative options:\n",
    "      # model=\"gpt-4o-mini\",           # Faster GPT-4 performance, good \n",
    "      # balance,\n",
    "      # model=\"gpt-3.5-turbo\",    # Faster and cheaper option\n",
    "\n",
    "      temperature=0         # 0 = deterministic, factual responses (recommended for Q&A)\n",
    "      # max_tokens is not a valid parameter here\n",
    "  )\n",
    "\n",
    "print(\"‚úì LLM configured successfully\")\n",
    "print(f\"  - Model: gpt-4o-mini\")\n",
    "print(f\"  - Temperature: 0 (deterministic)\")\n",
    "print(f\"  - Max tokens: 2000\")\n",
    "\n",
    "# Test the LLM with a simple query\n",
    "test_response = llm.invoke(\"Say 'Hello, I am ready to answer questions!'\")\n",
    "print(f\"\\nLLM Test Response: {test_response.content}\")\n",
    "\n",
    "#   üìù Explanation of Parameters:\n",
    "\n",
    "#   Model Selection:\n",
    "\n",
    "#   # Option 1: Best quality (slower, more expensive)\n",
    "#   llm = ChatOpenAI(model=\"gpt-4-turbo-2024-04-09\")\n",
    "\n",
    "#   # Option 2: Fast GPT-4 performance (balanced)\n",
    "#   llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "\n",
    "#   # Option 3: Fast and cheap (good for testing)\n",
    "#   llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "#   Temperature:\n",
    "\n",
    "#   temperature=0    # Deterministic, focused (best for factual Q&A)\n",
    "#   temperature=0.7  # More creative, varied responses\n",
    "#   temperature=1.0  # Most creative, less predictable\n",
    "\n",
    "#   Max Tokens:\n",
    "\n",
    "#   max_tokens=2000  # Controls maximum response length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Creating the RAG Chain (LangChain 1.0+ LCEL)\n",
    "\n",
    "### What is a RAG Chain?\n",
    "The RAG chain combines retrieval and generation into a single workflow:\n",
    "1. User asks a question\n",
    "2. Retriever finds relevant documents\n",
    "3. Documents are formatted as context\n",
    "4. LLM generates answer using the context\n",
    "\n",
    "### LangChain 1.0+ LCEL (LangChain Expression Language)\n",
    "LangChain 1.0+ uses LCEL, a declarative way to build chains using the pipe operator `|`.\n",
    "\n",
    "**Benefits:**\n",
    "- More intuitive and readable\n",
    "- Better streaming support\n",
    "- Easier to debug and modify\n",
    "- Type-safe and composable\n",
    "\n",
    "**Components:**\n",
    "- **RunnablePassthrough**: Passes input through unchanged\n",
    "- **Pipe operator (|)**: Chains components together\n",
    "- **StrOutputParser**: Converts LLM output to string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Retriever configured successfully\n",
      "  - Search type: similarity\n",
      "  - Number of documents to retrieve (k): 4\n",
      "\n",
      "--- Retriever Test ---\n",
      "Query: 'What is the main topic of this document?'\n",
      "Retrieved 4 documents:\n",
      "\n",
      "Document 1:\n",
      "  Content preview: 1\n",
      "Retrieval-Augmented Generation for Large\n",
      "Language Models: A Survey\n",
      "Yunfan Gaoa, Yun Xiongb, Xinyu Gao b, Kangxiang Jia b, Jinliu Pan b, Yuxi Bic, Yi...\n",
      "  Metadata: {'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'pdfs\\\\ragsurvey.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1'}\n",
      "\n",
      "Document 2:\n",
      "  Content preview: for continuous knowledge updates and integration of domain-\n",
      "specific information. RAG synergistically merges LLMs‚Äô intrin-\n",
      "sic knowledge with the vast...\n",
      "  Metadata: {'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'pdfs\\\\ragsurvey.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1'}\n",
      "\n",
      "Document 3:\n",
      "  Content preview: Jonathan Berant. Coarse-to-Ô¨Åne question answering for long documents. In Proceedings of the\n",
      "55th Annual Meeting of the Association for Computational L...\n",
      "  Metadata: {'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'author': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'pdfs\\\\rag.pdf', 'total_pages': 19, 'page': 9, 'page_label': '10'}\n",
      "\n",
      "Document 4:\n",
      "  Content preview: ‚Ä¢ In this survey, we present a thorough and systematic\n",
      "review of the state-of-the-art RAG methods, delineating\n",
      "its evolution through paradigms includi...\n",
      "  Metadata: {'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'pdfs\\\\ragsurvey.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1'}\n"
     ]
    }
   ],
   "source": [
    "#Step 1: Create the Retriever \n",
    "# Create a retriever from the vector store\n",
    "retriever = vectorstore.as_retriever(\n",
    "      search_type=\"similarity\",    # Use cosine similarity for search\n",
    "      search_kwargs={\"k\": 4}        # Retrieve top 4 most relevant chunks\n",
    "  )\n",
    "\n",
    "print(\"‚úì Retriever configured successfully\")\n",
    "print(f\"  - Search type: similarity\")\n",
    "print(f\"  - Number of documents to retrieve (k): 4\")\n",
    "\n",
    "  # Test the retriever with a sample query\n",
    "test_query = \"What is the main topic of this document?\"\n",
    "retrieved_docs = retriever.invoke(test_query)\n",
    "\n",
    "print(f\"\\n--- Retriever Test ---\")\n",
    "print(f\"Query: '{test_query}'\")\n",
    "print(f\"Retrieved {len(retrieved_docs)} documents:\")\n",
    "\n",
    "for i, doc in enumerate(retrieved_docs):\n",
    "    print(f\"\\nDocument {i+1}:\")\n",
    "    print(f\"  Content preview: {doc.page_content[:150]}...\")\n",
    "    print(f\"  Metadata: {doc.metadata}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì RAG chain created successfully using LangChain 1.0+ LCEL!\n",
      "\n",
      "RAG Pipeline Flow:\n",
      "  1. User provides a query\n",
      "  2. Retriever finds top 4 relevant chunks\n",
      "  3. Chunks are formatted as context\n",
      "  4. Context + question are formatted with prompt template\n",
      "  5. LLM generates answer based on context\n",
      "  6. Answer is parsed and returned to user\n"
     ]
    }
   ],
   "source": [
    "# Define the prompt template for the RAG system\n",
    "# This tells the LLM how to use the retrieved context\n",
    "system_prompt = (\n",
    "    \"You are a helpful assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer the question. \"\n",
    "    \"If you don't know the answer based on the context, say that you don't know. \"\n",
    "    \"Keep the answer concise and accurate.\\n\\n\"\n",
    "    \"Context: {context}\\n\\n\"\n",
    "    \"Question: {question}\"\n",
    ")\n",
    "\n",
    "# Create the prompt template\n",
    "prompt = ChatPromptTemplate.from_template(system_prompt)\n",
    "\n",
    "# Helper function to format documents\n",
    "def format_docs(docs):\n",
    "    \"\"\"Format retrieved documents into a single string.\"\"\"\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Build the RAG chain using LangChain 1.0+ LCEL (LangChain Expression Language)\n",
    "# This uses the pipe operator (|) to chain components together\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"context\": retriever | format_docs,  # Retrieve docs and format them\n",
    "        \"question\": RunnablePassthrough()      # Pass through the question\n",
    "    }\n",
    "    | prompt           # Format with prompt template\n",
    "    | llm              # Generate answer with LLM\n",
    "    | StrOutputParser() # Parse output to string\n",
    ")\n",
    "\n",
    "print(\"‚úì RAG chain created successfully using LangChain 1.0+ LCEL!\")\n",
    "print(\"\\nRAG Pipeline Flow:\")\n",
    "print(\"  1. User provides a query\")\n",
    "print(\"  2. Retriever finds top 4 relevant chunks\")\n",
    "print(\"  3. Chunks are formatted as context\")\n",
    "print(\"  4. Context + question are formatted with prompt template\")\n",
    "print(\"  5. LLM generates answer based on context\")\n",
    "print(\"  6. Answer is parsed and returned to user\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is the main topic or subject of this document?\n",
      "\n",
      "Processing...\n",
      "\n",
      "================================================================================\n",
      "ANSWER:\n",
      "================================================================================\n",
      "The main topic of the document is Retrieval-Augmented Generation (RAG) for knowledge-intensive natural language processing tasks, focusing on its implementation details, evaluation methods, and performance in various applications.\n",
      "\n",
      "================================================================================\n",
      "\n",
      "SOURCE DOCUMENTS USED:\n",
      "================================================================================\n",
      "\n",
      "Document 1:\n",
      "  Source: {'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'author': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'pdfs\\\\rag.pdf', 'total_pages': 19, 'page': 16, 'page_label': '17'}\n",
      "  Content: Appendices for Retrieval-Augmented Generation for\n",
      "Knowledge-Intensive NLP Tasks\n",
      "A Implementation Details\n",
      "For Open-domain QA we report test numbers using 15 retrieved documents for RAG-Token models.\n",
      "Fo...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Document 2:\n",
      "  Source: {'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'pdfs\\\\ragsurvey.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1'}\n",
      "  Content: 1\n",
      "Retrieval-Augmented Generation for Large\n",
      "Language Models: A Survey\n",
      "Yunfan Gaoa, Yun Xiongb, Xinyu Gao b, Kangxiang Jia b, Jinliu Pan b, Yuxi Bic, Yi Dai a, Jiawei Sun a, Meng\n",
      "Wangc, and Haofen Wang ...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Document 3:\n",
      "  Source: {'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'author': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'pdfs\\\\rag.pdf', 'total_pages': 19, 'page': 17, 'page_label': '18'}\n",
      "  Content: TriviaQA Web Development split. Roberts et al.[52] used the TriviaQA ofÔ¨Åcial Wikipedia test set\n",
      "instead. F√©vry et al. [14] follow this convention in order to compare with Roberts et al. [52] (See\n",
      "appe...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Document 4:\n",
      "  Source: {'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'pdfs\\\\ragsurvey.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1'}\n",
      "  Content: ‚Ä¢ In this survey, we present a thorough and systematic\n",
      "review of the state-of-the-art RAG methods, delineating\n",
      "its evolution through paradigms including naive RAG,\n",
      "arXiv:2312.10997v5  [cs.CL]  27 Mar ...\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Example Query 1: General question about the document\n",
    "query1 = \"What is the main topic or subject of this document?\"\n",
    "\n",
    "print(f\"Query: {query1}\")\n",
    "print(\"\\nProcessing...\\n\")\n",
    "\n",
    "# With LangChain 1.0+, we invoke the chain with the question directly\n",
    "answer = rag_chain.invoke(query1)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ANSWER:\")\n",
    "print(\"=\" * 80)\n",
    "print(answer)\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "# To see which documents were retrieved, we can call the retriever separately\n",
    "print(\"\\nSOURCE DOCUMENTS USED:\")\n",
    "print(\"=\" * 80)\n",
    "retrieved_docs = retriever.invoke(query1)\n",
    "for i, doc in enumerate(retrieved_docs):\n",
    "    print(f\"\\nDocument {i+1}:\")\n",
    "    print(f\"  Source: {doc.metadata}\")\n",
    "    print(f\"  Content: {doc.page_content[:200]}...\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Can you summarize the key points from this document?\n",
      "\n",
      "Processing...\n",
      "\n",
      "================================================================================\n",
      "ANSWER:\n",
      "================================================================================\n",
      "The document discusses the challenges and methodologies related to Retrieval-Augmented Generation (RAG) systems. Key points include:\n",
      "\n",
      "1. **Query Optimization**: Emphasizes the importance of formulating clear queries, as vague or complex questions can lead to ineffective retrieval. It highlights issues with language complexity and ambiguity.\n",
      "\n",
      "2. **Query Expansion**: Suggests expanding a single query into multiple queries to enhance retrieval effectiveness.\n",
      "\n",
      "3. **Reranking**: Describes the process of reordering document chunks to prioritize the most relevant results, using both rule-based and model-based methods.\n",
      "\n",
      "4. **Context Selection/Compression**: Addresses misconceptions about retrieving and concatenating documents, advocating for a streamlined approach that reduces redundancy and noise while ensuring relevance.\n",
      "\n",
      "5. **Task Adaptation**: Discusses the Task Adapter module, which customizes RAG for various tasks, improving precision and flexibility in information retrieval.\n",
      "\n",
      "6. **Training Setup**: Mentions the training of RAG models using Fairseq and the use of mixed precision on multiple GPUs, along with efficient document indexing using FAISS.\n",
      "\n",
      "Overall, the document outlines strategies to enhance the effectiveness of RAG systems in knowledge retrieval and reasoning tasks.\n",
      "\n",
      "================================================================================\n",
      "================================================================================\n",
      "ANSWER:\n",
      "================================================================================\n",
      "The document discusses the challenges and methodologies related to Retrieval-Augmented Generation (RAG) systems. Key points include:\n",
      "\n",
      "1. **Query Optimization**: Emphasizes the importance of formulating clear queries, as vague or complex questions can lead to ineffective retrieval. It highlights issues with language complexity and ambiguity.\n",
      "\n",
      "2. **Query Expansion**: Suggests expanding a single query into multiple queries to enhance retrieval effectiveness.\n",
      "\n",
      "3. **Reranking**: Describes the process of reordering document chunks to prioritize the most relevant results, using both rule-based and model-based methods.\n",
      "\n",
      "4. **Context Selection/Compression**: Addresses misconceptions about retrieving and concatenating documents, advocating for a streamlined approach that reduces redundancy and noise while ensuring relevance.\n",
      "\n",
      "5. **Task Adaptation**: Discusses the Task Adapter module, which customizes RAG for various tasks, improving precision and flexibility in information retrieval.\n",
      "\n",
      "6. **Training Setup**: Mentions the training of RAG models using Fairseq and the use of mixed precision on multiple GPUs, along with efficient document indexing using FAISS.\n",
      "\n",
      "Overall, the document outlines strategies to enhance the effectiveness of RAG systems in knowledge retrieval and reasoning tasks.\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Example Query 2: Specific information extraction\n",
    "query2 = \"Can you summarize the key points from this document?\"\n",
    "\n",
    "print(f\"Query: {query2}\")\n",
    "print(\"\\nProcessing...\\n\")\n",
    "\n",
    "answer = rag_chain.invoke(query2)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ANSWER:\")\n",
    "print(\"=\" * 80)\n",
    "print(answer)\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Please you Execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What specific details are mentioned about attention mechanisms?\n",
      "\n",
      "Processing...\n",
      "\n",
      "================================================================================\n",
      "ANSWER:\n",
      "================================================================================\n",
      "The context mentions several specific details about attention mechanisms:\n",
      "\n",
      "1. **Encoder-Decoder Attention**: In this mechanism, queries come from the previous decoder layer, while memory keys and values come from the encoder's output, allowing the decoder to attend to all positions in the input sequence.\n",
      "\n",
      "2. **Self-Attention in Encoder**: In self-attention layers of the encoder, all keys, values, and queries come from the same source, allowing each position to attend to all positions in the previous encoder layer.\n",
      "\n",
      "3. **Self-Attention in Decoder**: The decoder's self-attention layers allow each position to attend to all previous positions, preventing leftward information flow to maintain the auto-regressive property.\n",
      "\n",
      "4. **Multi-Head Attention**: The model employs multi-head attention, which allows it to attend to information from different representation subspaces at different positions. It uses multiple attention heads (h = 8) with reduced dimensions for computational efficiency.\n",
      "\n",
      "5. **Attention Formula**: The multi-head attention is defined mathematically, where each head computes attention using specific projection matrices.\n",
      "\n",
      "These details highlight the structure and functionality of attention mechanisms in the context of the Transformer model.\n",
      "\n",
      "================================================================================\n",
      "================================================================================\n",
      "ANSWER:\n",
      "================================================================================\n",
      "The context mentions several specific details about attention mechanisms:\n",
      "\n",
      "1. **Encoder-Decoder Attention**: In this mechanism, queries come from the previous decoder layer, while memory keys and values come from the encoder's output, allowing the decoder to attend to all positions in the input sequence.\n",
      "\n",
      "2. **Self-Attention in Encoder**: In self-attention layers of the encoder, all keys, values, and queries come from the same source, allowing each position to attend to all positions in the previous encoder layer.\n",
      "\n",
      "3. **Self-Attention in Decoder**: The decoder's self-attention layers allow each position to attend to all previous positions, preventing leftward information flow to maintain the auto-regressive property.\n",
      "\n",
      "4. **Multi-Head Attention**: The model employs multi-head attention, which allows it to attend to information from different representation subspaces at different positions. It uses multiple attention heads (h = 8) with reduced dimensions for computational efficiency.\n",
      "\n",
      "5. **Attention Formula**: The multi-head attention is defined mathematically, where each head computes attention using specific projection matrices.\n",
      "\n",
      "These details highlight the structure and functionality of attention mechanisms in the context of the Transformer model.\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Example Query 3: Your custom question\n",
    "# Replace this with your own question!\n",
    "custom_query = \"What specific details are mentioned about attention mechanisms?\"\n",
    "\n",
    "print(f\"Query: {custom_query}\")\n",
    "print(\"\\nProcessing...\\n\")\n",
    "\n",
    "answer = rag_chain.invoke(custom_query)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ANSWER:\")\n",
    "print(\"=\" * 80)\n",
    "print(answer)\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What are the applications of Attention Mechanism?\n",
      "\n",
      "Processing...\n",
      "\n",
      "================================================================================\n",
      "ANSWER:\n",
      "================================================================================\n",
      "The attention mechanism is integral to sequence modeling and transduction models in various tasks, allowing for the modeling of dependencies without regard to their distance in the input or output sequences. It is commonly used in natural language processing tasks such as machine translation, text summarization, and other applications that require understanding relationships in sequential data.\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Example Query 3: Your custom question\n",
    "# Replace this with your own question!\n",
    "custom_query = \"What are the applications of Attention Mechanism?\"\n",
    "\n",
    "print(f\"Query: {custom_query}\")\n",
    "print(\"\\nProcessing...\\n\")\n",
    "\n",
    "response3 = rag_chain.invoke(custom_query)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ANSWER:\")\n",
    "print(\"=\" * 80)\n",
    "print(response3)\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FAISS Kernel Crash ISSUE\n",
    "\n",
    "Testing retrieval with query: 'What is the main topic of this document?'\n",
    "OMP: Error #15: Initializing libomp.dylib, but found libomp.dylib already initialized.\n",
    "OMP: Hint This means that multiple copies of the OpenMP runtime have been linked into the program. That is dangerous, since it can degrade performance or cause incorrect results. The best thing to do is to ensure that only a single OpenMP runtime is linked into the process, e.g. by avoiding static linking of the OpenMP runtime in any library. As an unsafe, unsupported, undocumented workaround you can set the environment variable KMP_DUPLICATE_LIB_OK=TRUE to allow the program to continue to execute, but that may cause crashes or silently produce incorrect results. For more information, please see http://openmp.llvm.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lcrag_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
