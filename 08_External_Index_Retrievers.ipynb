{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 08 - External Index Retrievers üåê\n",
    "\n",
    "## Learning Objectives üéØ\n",
    "\n",
    "In this notebook, you'll learn:\n",
    "\n",
    "1. **What are External Index Retrievers** and how they differ from vector store retrievers\n",
    "2. **ArxivRetriever** - Search and retrieve scholarly articles from arxiv.org\n",
    "3. **WikipediaRetriever** - Access Wikipedia articles for general knowledge\n",
    "4. **TavilySearchAPIRetriever** - Perform real-time internet searches\n",
    "5. **Integration with RAG Chains** - Combine external retrievers with LLMs\n",
    "6. **Best Practices** - When and how to use each retriever effectively\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents üìö\n",
    "\n",
    "1. [Introduction to External Retrievers](#intro)\n",
    "2. [Setup & Installation](#setup)\n",
    "3. [ArxivRetriever - Academic Papers](#arxiv)\n",
    "4. [WikipediaRetriever - General Knowledge](#wikipedia)\n",
    "5. [TavilySearchAPIRetriever - Web Search](#tavily)\n",
    "6. [Integration with RAG Chains](#rag)\n",
    "7. [Comparison & Use Cases](#comparison)\n",
    "8. [Best Practices](#best-practices)\n",
    "9. [Summary & Exercises](#summary)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='intro'></a>\n",
    "## 1. Introduction to External Index Retrievers üîç\n",
    "\n",
    "### What are External Index Retrievers?\n",
    "\n",
    "**External Index Retrievers** search over external data sources (e.g., the internet, academic databases, knowledge bases) rather than your local vector store.\n",
    "\n",
    "### Key Differences:\n",
    "\n",
    "| Feature | Vector Store Retrievers | External Index Retrievers |\n",
    "|---------|------------------------|---------------------------|\n",
    "| **Data Source** | Your embedded documents | External databases/APIs |\n",
    "| **Data Freshness** | Static (at indexing time) | Real-time or regularly updated |\n",
    "| **Setup Required** | Embedding + Vector store | API keys (sometimes) |\n",
    "| **Use Cases** | Internal documents, knowledge bases | Current events, academic research, general knowledge |\n",
    "| **Cost** | Embedding cost + storage | API calls (often free tier available) |\n",
    "\n",
    "### When to Use External Retrievers:\n",
    "\n",
    "- ‚úÖ You need **up-to-date information** from the internet\n",
    "- ‚úÖ You want to access **specialized databases** (e.g., academic papers)\n",
    "- ‚úÖ You need **general knowledge** without building a custom knowledge base\n",
    "- ‚úÖ You want to **augment** your local data with external sources\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='setup'></a>\n",
    "## 2. Setup & Installation ‚öôÔ∏è\n",
    "\n",
    "### Required Packages\n",
    "\n",
    "All external retrievers are part of `langchain-community`. You'll also need:\n",
    "\n",
    "```bash\n",
    "uv pip install langchain-community\n",
    "uv pip install arxiv           # For ArxivRetriever\n",
    "uv pip install wikipedia       # For WikipediaRetriever\n",
    "uv pip install tavily-python   # For TavilySearchAPIRetriever\n",
    "```\n",
    "\n",
    "### Environment Variables\n",
    "\n",
    "For TavilySearchAPIRetriever, you'll need an API key:\n",
    "\n",
    "```\n",
    "TAVILY_API_KEY=your_api_key_here\n",
    "```\n",
    "\n",
    "Get your free API key at: https://tavily.com/\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LangChain version: 1.1.0\n",
      "‚úÖ Setup complete!\n"
     ]
    }
   ],
   "source": [
    "# Setup: Import required libraries\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Import LangChain components\n",
    "from langchain_community.retrievers import ArxivRetriever, WikipediaRetriever, TavilySearchAPIRetriever\n",
    "from langchain_core.documents import Document\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Verify versions\n",
    "import langchain\n",
    "print(f\"‚úÖ LangChain version: {langchain.__version__}\")\n",
    "print(\"‚úÖ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='arxiv'></a>\n",
    "## 3. ArxivRetriever - Academic Papers üìÑ\n",
    "\n",
    "### üî∞ BEGINNER: What is ArxivRetriever?\n",
    "\n",
    "**ArxivRetriever** searches [arxiv.org](https://arxiv.org), a repository of electronic preprints for research papers in:\n",
    "- Physics\n",
    "- Mathematics\n",
    "- Computer Science\n",
    "- Quantitative Biology\n",
    "- Quantitative Finance\n",
    "- Statistics\n",
    "\n",
    "### Use Cases:\n",
    "- üìö Literature review for research\n",
    "- üß† Getting latest research on AI/ML topics\n",
    "- üìä Finding papers by specific authors\n",
    "- üî¨ Accessing cutting-edge research\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üî∞ BEGINNER: Basic ArxivRetriever Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.13.5 environment at: lcrag_venv\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 13ms\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv pip install arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö Found 3 papers on 'large language models'\n",
      "\n",
      "================================================================================\n",
      "Title: Large Language Models Lack Understanding of Character Composition of Words\n",
      "Authors: Andrew Shin, Kunitake Kaneko\n",
      "Published: 2024-07-23\n",
      "\n",
      "Abstract (first 500 chars):\n",
      "Large language models (LLMs) have demonstrated remarkable performances on a wide range of natural language tasks. Yet, LLMs' successes have been largely restricted to tasks concerning words, sentences, or documents, and it remains questionable how much they understand the minimal units of text, namely characters. In this paper, we examine contemporary LLMs regarding their ability to understand character composition of words, and show that most of them fail to reliably carry out even the simple t...\n",
      "================================================================================\n",
      "Title: Is Self-knowledge and Action Consistent or Not: Investigating Large Language Model's Personality\n",
      "================================================================================\n",
      "Title: Unmasking the Shadows of AI: Investigating Deceptive Capabilities in Large Language Models\n"
     ]
    }
   ],
   "source": [
    "import arxiv\n",
    "\n",
    "# Create an ArxivRetriever instance\n",
    "# By default, it returns top 3 documents\n",
    "arxiv_retriever = ArxivRetriever(\n",
    "\tload_max_docs=3,\n",
    "\tarxiv_search=arxiv.Search,\n",
    "\tarxiv_exceptions=arxiv.ArxivError\n",
    ")\n",
    "\n",
    "# Search for papers on \"large language models\"\n",
    "query = \"large language models\"\n",
    "docs = arxiv_retriever.invoke(query)\n",
    "\n",
    "print(f\"üìö Found {len(docs)} papers on '{query}'\\n\")\n",
    "\n",
    "# Display first paper\n",
    "print(\"=\" * 80)\n",
    "print(f\"Title: {docs[0].metadata.get('Title', 'N/A')}\")\n",
    "print(f\"Authors: {docs[0].metadata.get('Authors', 'N/A')}\")\n",
    "print(f\"Published: {docs[0].metadata.get('Published', 'N/A')}\")\n",
    "print(f\"\\nAbstract (first 500 chars):\\n{docs[0].page_content[:500]}...\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Title: {docs[1].metadata.get('Title', 'N/A')}\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Title: {docs[2].metadata.get('Title', 'N/A')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéì INTERMEDIATE: Advanced ArxivRetriever Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö Retrieved 3 papers\n",
      "\n",
      "1. Vision Transformer with Quadrangle Attention\n",
      "   Authors: Qiming Zhang, Jing Zhang, Yufei Xu, Dacheng Tao\n",
      "   Published: 2023-03-27\n",
      "   Entry ID: N/A\n",
      "\n",
      "2. D√©j√† vu: A Contextualized Temporal Attention Mechanism for Sequential Recommendation\n",
      "   Authors: Jibang Wu, Renqin Cai, Hongning Wang\n",
      "   Published: 2020-01-29\n",
      "   Entry ID: N/A\n",
      "\n",
      "3. Self-Attention as Distributional Projection: A Unified Interpretation of Transformer Architecture\n",
      "   Authors: Nihal Mehta\n",
      "   Published: 2025-11-16\n",
      "   Entry ID: N/A\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Advanced: Retrieve more documents and explore metadata\n",
    "arxiv_retriever_advanced = ArxivRetriever(\n",
    "    load_max_docs=5,  # Get top 5 papers\n",
    "    load_all_available_meta=True,  # Load all metadata\n",
    "    arxiv_search=arxiv.Search,\n",
    "    arxiv_exceptions=arxiv.ArxivError\n",
    ")\n",
    "\n",
    "# Search for papers on \"transformers attention mechanism\"\n",
    "query = \"transformers attention mechanism\"\n",
    "docs = arxiv_retriever_advanced.invoke(query)\n",
    "\n",
    "print(f\"üìö Retrieved {len(docs)} papers\\n\")\n",
    "\n",
    "# Display metadata for all papers\n",
    "for i, doc in enumerate(docs, 1):\n",
    "    print(f\"{i}. {doc.metadata.get('Title', 'N/A')}\")\n",
    "    print(f\"   Authors: {doc.metadata.get('Authors', 'N/A')}\")\n",
    "    print(f\"   Published: {doc.metadata.get('Published', 'N/A')}\")\n",
    "    print(f\"   Entry ID: {doc.metadata.get('entry_id', 'N/A')}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéì INTERMEDIATE: Using .batch() for Multiple Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö Batch Search Results:\n",
      "\n",
      "Query: 'RAG retrieval augmented generation'\n",
      "  ‚Üí Found 3 papers\n",
      "  ‚Üí Top result: AR-RAG: Autoregressive Retrieval Augmentation for Image Generation\n",
      "\n",
      "Query: 'vector embeddings'\n",
      "  ‚Üí Found 3 papers\n",
      "  ‚Üí Top result: Part-of-Speech Relevance Weights for Learning Word Embeddings\n",
      "\n",
      "Query: 'prompt engineering'\n",
      "  ‚Üí Found 3 papers\n",
      "  ‚Üí Top result: Towards Goal-oriented Prompt Engineering for Large Language Models: A Survey\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Batch processing: Search multiple topics at once\n",
    "queries = [\n",
    "    \"RAG retrieval augmented generation\",\n",
    "    \"vector embeddings\",\n",
    "    \"prompt engineering\"\n",
    "]\n",
    "\n",
    "arxiv_retriever_batch = ArxivRetriever(\n",
    "    load_max_docs=3,\n",
    "    arxiv_search=arxiv.Search,\n",
    "    arxiv_exceptions=arxiv.ArxivError\n",
    ")\n",
    "batch_results = arxiv_retriever_batch.batch(queries)  # Instead of single invoke call for each query \n",
    "                                                       # we use batch to process all at once\n",
    "                                                       #  \n",
    "\n",
    "print(\"üìö Batch Search Results:\\n\")\n",
    "for query, docs in zip(queries, batch_results):\n",
    "    print(f\"Query: '{query}'\")\n",
    "    print(f\"  ‚Üí Found {len(docs)} papers\")\n",
    "    if docs:\n",
    "        print(f\"  ‚Üí Top result: {docs[0].metadata.get('Title', 'N/A')}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìä Understanding ArxivRetriever Metadata\n",
    "\n",
    "Each document returned by ArxivRetriever contains rich metadata:\n",
    "\n",
    "```python\n",
    "{\n",
    "    'Published': '2023-06-15',           # Publication date\n",
    "    'Title': 'Paper Title',              # Full title\n",
    "    'Authors': 'Author1, Author2',       # Comma-separated authors\n",
    "    'Summary': 'Abstract text...',       # Paper abstract/summary\n",
    "    'entry_id': 'http://arxiv.org/...',  # Arxiv URL\n",
    "}\n",
    "```\n",
    "\n",
    "The `page_content` field contains the full abstract/summary of the paper.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='wikipedia'></a>\n",
    "## 4. WikipediaRetriever - General Knowledge üìñ\n",
    "\n",
    "### üî∞ BEGINNER: What is WikipediaRetriever?\n",
    "\n",
    "**WikipediaRetriever** searches and retrieves content from Wikipedia, the free encyclopedia with 6+ million articles.\n",
    "\n",
    "### Use Cases:\n",
    "- üåç General knowledge questions\n",
    "- üìö Quick facts and definitions\n",
    "- üèõÔ∏è Historical information\n",
    "- üßë‚Äçüî¨ Biographical data\n",
    "- üó∫Ô∏è Geographic information\n",
    "\n",
    "### Important Notes:\n",
    "- ‚ö†Ô∏è Wikipedia content is **community-edited** - verify critical information\n",
    "- ‚úÖ Great for general knowledge, not for specialized or proprietary data\n",
    "- üåê Supports multiple languages\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üî∞ BEGINNER: Basic WikipediaRetriever Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[2mResolved \u001b[1m9 packages\u001b[0m \u001b[2min 1.21s\u001b[0m\u001b[0m                                         \u001b[0m\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m1 package\u001b[0m \u001b[2min 486ms\u001b[0m\u001b[0m                                              \n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 1ms\u001b[0m\u001b[0m                                  \u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mwikipedia\u001b[0m\u001b[2m==1.4.0\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv pip install wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìñ Found 2 Wikipedia articles on 'Python programming language'\n",
      "\n",
      "================================================================================\n",
      "Title: Python (programming language)\n",
      "Source: https://en.wikipedia.org/wiki/Python_(programming_language)\n",
      "\n",
      "Content (first 600 chars):\n",
      "Python is a high-level, general-purpose programming language. Its design philosophy emphasizes code readability with the use of significant indentation. Python is dynamically type-checked and garbage-collected. It supports multiple programming paradigms, including structured (particularly procedural), object-oriented and functional programming.\n",
      "Guido van Rossum began working on Python in the late 1980s as a successor to the ABC programming language. Python 3.0, released in 2008, was a major revision and not completely backward-compatible with earlier versions. Beginning with Python 3.5, capabi...\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Create a WikipediaRetriever instance\n",
    "# By default, it returns top 3 documents\n",
    "wiki_retriever = WikipediaRetriever(top_k_results=2, wiki_client=None)\n",
    "\n",
    "# Search for information on \"Python programming language\"\n",
    "query = \"Python programming language\"\n",
    "docs = wiki_retriever.invoke(query)\n",
    "\n",
    "print(f\"üìñ Found {len(docs)} Wikipedia articles on '{query}'\\n\")\n",
    "\n",
    "# Display first result\n",
    "print(\"=\" * 80)\n",
    "print(f\"Title: {docs[0].metadata.get('title', 'N/A')}\")\n",
    "print(f\"Source: {docs[0].metadata.get('source', 'N/A')}\")\n",
    "print(f\"\\nContent (first 600 chars):\\n{docs[0].page_content[:600]}...\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéì INTERMEDIATE: Advanced WikipediaRetriever Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìñ Retrieved 3 Wikipedia articles\n",
      "\n",
      "1. Title: Machine learning\n",
      "   Summary: Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn...\n",
      "   Content length: 1000 characters\n",
      "\n",
      "2. Title: Neural network (machine learning)\n",
      "   Summary: In machine learning, a neural network or neural net (NN), also called artificial neural network (ANN), is a computational model inspired by the struct...\n",
      "   Content length: 1000 characters\n",
      "\n",
      "3. Title: Attention (machine learning)\n",
      "   Summary: In machine learning, attention is a method that determines the importance of each component in a sequence relative to the other components in that seq...\n",
      "   Content length: 1000 characters\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Advanced: Control number of results and document length\n",
    "wiki_retriever_advanced = WikipediaRetriever(\n",
    "    top_k_results=3,        # Get top 3 results\n",
    "    doc_content_chars_max=1000,  # Limit content to 1000 characters per doc\n",
    "    wiki_client=None\n",
    ")\n",
    "\n",
    "# Search for \"Machine Learning\"\n",
    "query = \"Machine Learning\"\n",
    "docs = wiki_retriever_advanced.invoke(query)\n",
    "\n",
    "print(f\"üìñ Retrieved {len(docs)} Wikipedia articles\\n\")\n",
    "\n",
    "# Display all results\n",
    "for i, doc in enumerate(docs, 1):\n",
    "    print(f\"{i}. Title: {doc.metadata.get('title', 'N/A')}\")\n",
    "    print(f\"   Summary: {doc.metadata.get('summary', 'N/A')[:150]}...\")\n",
    "    print(f\"   Content length: {len(doc.page_content)} characters\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéì INTERMEDIATE: Multilingual Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåê Search in Spanish Wikipedia: 'Inteligencia Artificial'\n",
      "\n",
      "Title: Inteligencia artificial\n",
      "Content preview:\n",
      "La inteligencia artificial, abreviado como IA, en el contexto de las ciencias de la computaci√≥n, es una disciplina y un conjunto de capacidades cognoscitivas e intelectuales expresadas por sistemas inform√°ticos o combinaciones de algoritmos cuyo prop√≥sito es la creaci√≥n de m√°quinas que imiten la inteligencia humana.\n",
      "Estas tecnolog√≠as permiten que las m√°quinas aprendan de la experiencia, se adapten...\n"
     ]
    }
   ],
   "source": [
    "# Search in different languages\n",
    "# Default is English ('en'), but you can specify other languages\n",
    "\n",
    "# Example: Search in Spanish\n",
    "wiki_retriever_es = WikipediaRetriever(\n",
    "    top_k_results=1,\n",
    "    lang=\"es\",  # Spanish Wikipedia\n",
    "    wiki_client=None\n",
    ")\n",
    "\n",
    "query = \"Inteligencia Artificial\"\n",
    "docs = wiki_retriever_es.invoke(query)\n",
    "\n",
    "print(f\"üåê Search in Spanish Wikipedia: '{query}'\\n\")\n",
    "print(f\"Title: {docs[0].metadata.get('title', 'N/A')}\")\n",
    "print(f\"Content preview:\\n{docs[0].page_content[:400]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéì INTERMEDIATE: Batch Processing with WikipediaRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìñ Batch Wikipedia Search Results:\n",
      "\n",
      "Query: 'Albert Einstein'\n",
      "  ‚Üí Title: Albert Einstein\n",
      "  ‚Üí Summary: Albert Einstein (14 March 1879 ‚Äì 18 April 1955) was a German-born theoretical physicist best known for developing the theory of relativity. Einstein also made important contributions to quantum theory...\n",
      "\n",
      "Query: 'Quantum Computing'\n",
      "  ‚Üí Title: Quantum computing\n",
      "  ‚Üí Summary: A quantum computer is a (real or theoretical) computer that exploits superposed and entangled states, and the intrinsically non-deterministic outcomes of quantum measurements, as features of its compu...\n",
      "\n",
      "Query: 'Neural Networks'\n",
      "  ‚Üí Title: Neural network (machine learning)\n",
      "  ‚Üí Summary: In machine learning, a neural network or neural net (NN), also called artificial neural network (ANN), is a computational model inspired by the structure and functions of biological neural networks.\n",
      "A...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Batch search for multiple topics\n",
    "queries = [\n",
    "    \"Albert Einstein\",\n",
    "    \"Quantum Computing\",\n",
    "    \"Neural Networks\"\n",
    "]\n",
    "\n",
    "wiki_retriever_batch = WikipediaRetriever(top_k_results=1, doc_content_chars_max=500, wiki_client=None)\n",
    "batch_results = wiki_retriever_batch.batch(queries)\n",
    "\n",
    "print(\"üìñ Batch Wikipedia Search Results:\\n\")\n",
    "for query, docs in zip(queries, batch_results):\n",
    "    print(f\"Query: '{query}'\")\n",
    "    if docs:\n",
    "        print(f\"  ‚Üí Title: {docs[0].metadata.get('title', 'N/A')}\")\n",
    "        print(f\"  ‚Üí Summary: {docs[0].page_content[:200]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìä Understanding WikipediaRetriever Metadata\n",
    "\n",
    "Each document returned by WikipediaRetriever contains:\n",
    "\n",
    "```python\n",
    "{\n",
    "    'title': 'Article Title',           # Wikipedia article title\n",
    "    'summary': 'Brief summary...',       # Short summary (if available)\n",
    "    'source': 'https://en.wikipedia...', # Full Wikipedia URL\n",
    "}\n",
    "```\n",
    "\n",
    "The `page_content` field contains the article text (up to `doc_content_chars_max` characters).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='tavily'></a>\n",
    "## 5. TavilySearchAPIRetriever - Web Search üîç\n",
    "\n",
    "### üî∞ BEGINNER: What is TavilySearchAPIRetriever?\n",
    "\n",
    "**TavilySearchAPIRetriever** performs **real-time internet searches** using the Tavily Search API, optimized for AI applications.\n",
    "\n",
    "### Key Features:\n",
    "- üåê **Real-time web search** - Get the latest information from the internet\n",
    "- üéØ **AI-optimized** - Returns clean, relevant content for LLMs\n",
    "- üîí **Source attribution** - Includes URLs and metadata\n",
    "- ‚ö° **Fast & reliable** - Built specifically for AI use cases\n",
    "\n",
    "### Use Cases:\n",
    "- üì∞ Current events and news\n",
    "- üíπ Stock prices and market data\n",
    "- üå¶Ô∏è Weather information\n",
    "- üè¢ Company information\n",
    "- üîß Technical documentation and tutorials\n",
    "\n",
    "### Getting Started:\n",
    "1. Sign up at https://tavily.com/ (free tier available)\n",
    "2. Get your API key\n",
    "3. Add to `.env` file: `TAVILY_API_KEY=your_api_key`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üî∞ BEGINNER: Basic TavilySearchAPIRetriever Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.13.5 environment at: lcrag_venv\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 15ms\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv pip install tavily-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Found 3 web results for 'latest developments in artificial intelligence 2024'\n",
      "\n",
      "================================================================================\n",
      "Source: https://www.launchconsulting.com/posts/the-future-of-business-ai-innovations-to-watch-in-2024\n",
      "\n",
      "Content (first 500 chars):\n",
      "AI Trends in 2024 ¬∑ 1. Generative AI: Beyond Chatbots ¬∑ 2. The Emergence of Small Language Models ¬∑ 3. Multi-Modal AI Experiences ¬∑ 4. AI Empowerment for...\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Create a TavilySearchAPIRetriever instance\n",
    "# Make sure TAVILY_API_KEY is set in your .env file\n",
    "\n",
    "tavily_retriever = TavilySearchAPIRetriever(k=3)  # Return top 3 results\n",
    "\n",
    "# Search for \"latest developments in artificial intelligence 2024\"\n",
    "query = \"latest developments in artificial intelligence 2024\"\n",
    "docs = tavily_retriever.invoke(query)\n",
    "\n",
    "print(f\"üîç Found {len(docs)} web results for '{query}'\\n\")\n",
    "\n",
    "# Display first result\n",
    "print(\"=\" * 80)\n",
    "print(f\"Source: {docs[0].metadata.get('source', 'N/A')}\")\n",
    "print(f\"\\nContent (first 500 chars):\\n{docs[0].page_content[:500]}...\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéì INTERMEDIATE: Advanced TavilySearchAPIRetriever Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Retrieved 5 web results\n",
      "\n",
      "1. Source: https://www.youtube.com/watch?v=yF9kGESAi3M\n",
      "   Content preview: LangChain Master Class For Beginners 2024 [+20 Examples, LangChain V0.2]\n",
      "aiwithbrandon\n",
      "84100 subscribers\n",
      "12977 likes\n",
      "689864 views\n",
      "22 Jun 2024\n",
      "üöÄ Pre-order Shipkit.ai - AI dev toolkit for AI-driven deve...\n",
      "\n",
      "2. Source: https://www.datacamp.com/tutorial/how-to-build-llm-applications-with-langchain\n",
      "   Content preview: Explore the untapped potential of Large Language Models with LangChain, an open-source Python framework for building advanced AI applications. Here, we explore LangChain - An open-source Python framew...\n",
      "\n",
      "3. Source: https://github.com/gkamradt/langchain-tutorials\n",
      "   Content preview: 2. LangChain CookBook Part 2: 9 Use Cases - Code, Video | Kor | Eugene Yurtsev | üêí Intermediate | ‚úÖ Code | This is a half-baked prototype that ‚Äúhelps‚Äù you extract structured data from text using large...\n",
      "\n",
      "4. Source: https://langchain-5e9cc07a.mintlify.app/oss/python/learn\n",
      "   Content preview: Below are tutorials for common use cases, organized by framework. ‚Äã. LangChain. LangChain agent implementations make it easy to get started for most use cases....\n",
      "\n",
      "5. Source: https://www.geeksforgeeks.org/artificial-intelligence/introduction-to-langchain/\n",
      "   Content preview: LangChain is an open-source framework designed to simplify the creation of applications using large language models (LLMs)....\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Advanced: Control search depth and domain filtering\n",
    "from langchain_community.retrievers import TavilySearchAPIRetriever\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# If you want to set the API key directly in the notebook (for testing only), uncomment and set your key:\n",
    "# import os\n",
    "os.environ[\"TAVILY_API_KEY\"] = \"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\"\n",
    "\n",
    "# TavilyClient is not needed; TavilySearchAPIRetriever handles API key automatically.\n",
    "\n",
    "# Advanced configuration\n",
    "tavily_retriever_advanced = TavilySearchAPIRetriever(\n",
    "    k=5,  # Return top 5 results\n",
    "    # search_depth=\"advanced\",  # \"basic\" or \"advanced\" (more thorough)\n",
    "    # include_domains=[\"github.com\", \"stackoverflow.com\"],  # Filter to specific domains\n",
    "    # exclude_domains=[\"example.com\"]  # Exclude specific domains\n",
    ")\n",
    "\n",
    "# Search for \"LangChain tutorials\"\n",
    "query = \"LangChain tutorials\"\n",
    "docs = tavily_retriever_advanced.invoke(query)\n",
    "print(f\"üîç Retrieved {len(docs)} web results\\n\")\n",
    "\n",
    "# Display all results with sources\n",
    "for i, doc in enumerate(docs, 1):\n",
    "    print(f\"{i}. Source: {doc.metadata.get('source', 'N/A')}\")\n",
    "    print(f\"   Content preview: {doc.page_content[:200]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéì INTERMEDIATE: Real-Time Information Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üïê Real-Time Information (as of November 23, 2025):\n",
      "\n",
      "Query: 'latest AI news November 23, 2025'\n",
      "  ‚Üí Catch up on select AI news and developments from the past week or so: Google launches Gemini 3 and bakes it into search from Day One....\n",
      "  ‚Üí Source: https://www.marketingprofs.com/opinions/2025/54030/ai-update-november-21-2025-ai-news-and-views-from-the-past-week\n",
      "\n",
      "Query: 'current weather in San Francisco'\n",
      "  ‚Üí {'location': {'name': 'San Francisco', 'region': 'California', 'country': 'United States of America', 'lat': 37.775, 'lon': -122.4183, 'tz_id': 'America/Los_Angeles', 'localtime_epoch': 1763878604, 'localtime': '2025-11-22 22:16'}, 'current': {'last_...\n",
      "  ‚Üí Source: https://www.weatherapi.com/\n",
      "\n",
      "Query: 'NVIDIA stock price today'\n",
      "  ‚Üí The NVIDIA Corporation Common Stock (NVDA) stock price today is $180.03, reflecting a -0.49% move since the market opened. The company's market capitalization...\n",
      "  ‚Üí Source: https://www.kraken.com/stocks/nvda\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example: Get current information (news, weather, stock prices, etc.)\n",
    "from datetime import datetime\n",
    "\n",
    "current_date = datetime.now().strftime(\"%B %d, %Y\")\n",
    "\n",
    "# Real-time queries\n",
    "queries = [\n",
    "    f\"latest AI news {current_date}\",\n",
    "    \"current weather in San Francisco\",\n",
    "    \"NVIDIA stock price today\"\n",
    "]\n",
    "\n",
    "tavily_realtime = TavilySearchAPIRetriever(k=2)\n",
    "\n",
    "print(f\"üïê Real-Time Information (as of {current_date}):\\n\")\n",
    "\n",
    "for query in queries:\n",
    "    docs = tavily_realtime.invoke(query)\n",
    "    print(f\"Query: '{query}'\")\n",
    "    if docs:\n",
    "        print(f\"  ‚Üí {docs[0].page_content[:250]}...\")\n",
    "        print(f\"  ‚Üí Source: {docs[0].metadata.get('source', 'N/A')}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìä Understanding TavilySearchAPIRetriever Metadata\n",
    "\n",
    "Each document returned by TavilySearchAPIRetriever contains:\n",
    "\n",
    "```python\n",
    "{\n",
    "    'source': 'https://example.com/...',  # Source URL\n",
    "    'score': 0.95,                         # Relevance score (0-1)\n",
    "    'title': 'Page Title',                 # Web page title (if available)\n",
    "}\n",
    "```\n",
    "\n",
    "The `page_content` field contains the extracted text content from the web page.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='rag'></a>\n",
    "## 6. Integration with RAG Chains üîó\n",
    "\n",
    "Now let's combine external retrievers with LLMs to build powerful **Retrieval-Augmented Generation (RAG)** systems!\n",
    "\n",
    "### üî∞ BEGINNER: Simple QA Chain with External Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is quantum computing and how does it work?\n",
      "\n",
      "Answer: Quantum computing is a type of computing that utilizes the principles of quantum mechanics, specifically superposition and entanglement, to process information. Unlike classical computers, which operate on deterministic rules and use bits as the basic unit of information (where each bit can be either 0 or 1), quantum computers use qubits. A qubit can exist in a superposition of both states simultaneously, allowing quantum computers to explore a vast number of possibilities at once.\n",
      "\n",
      "The operation of a quantum computer involves manipulating qubits in such a way that their quantum states can interfere with one another. This interference can amplify the probability of obtaining the desired measurement result when the qubits are measured. The design of quantum algorithms focuses on creating procedures that leverage this amplification effect to perform calculations more efficiently than classical algorithms.\n",
      "\n",
      "Quantum computers are believed to have the potential to solve certain problems exponentially faster than classical computers, such as breaking widely used public-key cryptographic schemes or aiding in complex physical simulations. However, current quantum computing hardware is still largely experimental and is primarily suitable for specialized tasks rather than practical, real-world applications.\n"
     ]
    }
   ],
   "source": [
    "# Build a simple RAG chain using WikipediaRetriever\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Initialize components\n",
    "wiki_retriever = WikipediaRetriever(top_k_results=2, doc_content_chars_max=2000, wiki_client=None)\n",
    "# llm = ChatOpenAI(model=\"gpt-5-nano\", temperature=0)  DO NOT Have access to gpt-5-nano\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# Create prompt template\n",
    "template = \"\"\"Answer the question based on the following context from Wikipedia:\n",
    "\n",
    "# Context will be coming from extrenal retriever \n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Helper function to format documents\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Build the RAG chain using LCEL _ Langchain Expression Language\n",
    "rag_chain = (\n",
    "    {\"context\": wiki_retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Ask a question\n",
    "question = \"What is quantum computing and how does it work?\"\n",
    "answer = rag_chain.invoke(question)\n",
    "\n",
    "print(f\"Question: {question}\\n\")\n",
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéì INTERMEDIATE: Multi-Source RAG Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What are transformers in machine learning?\n",
      "\n",
      "Answer (from multiple sources):\n",
      "Transformers are a type of artificial neural network architecture that has revolutionized the field of machine learning, particularly in natural language processing (NLP) and other domains requiring sequential data analysis. Introduced in the landmark paper \"Attention Is All You Need\" by researchers at Google in 2017, transformers utilize a mechanism known as multi-head attention to process input data more effectively than previous architectures, such as recurrent neural networks (RNNs).\n",
      "\n",
      "At the core of the transformer architecture is the concept of attention, which allows the model to weigh the importance of different tokens (words or elements) in a sequence when making predictions. This is achieved through a process where each token is converted into a numerical representation called a vector, using a word embedding table. The multi-head attention mechanism enables the model to consider multiple aspects of the input data simultaneously, contextualizing each token within the broader context of the sequence. This parallel processing capability significantly enhances the model's ability to capture complex relationships and dependencies in the data.\n",
      "\n",
      "Transformers have become foundational in modern AI applications, powering a wide range of systems, including large language models that generate human-like text, perform translations, and engage in dialogue. Their flexibility and efficiency in handling large datasets have also made them valuable in other fields, such as biology, where machine learning techniques are increasingly applied to improve decision-making and predictions.\n",
      "\n",
      "In summary, transformers represent a significant advancement in machine learning, characterized by their use of attention mechanisms and parallel processing capabilities, which allow for more effective handling of sequential data and have led to breakthroughs in various AI applications.\n"
     ]
    }
   ],
   "source": [
    "# Advanced: Combine multiple retrievers for comprehensive answers\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "# Initialize multiple retrievers\n",
    "import arxiv\n",
    "\n",
    "arxiv_retriever = ArxivRetriever(\n",
    "    load_max_docs=2,\n",
    "    arxiv_search=arxiv.Search,\n",
    "    arxiv_exceptions=arxiv.ArxivError\n",
    ")\n",
    "wiki_retriever = WikipediaRetriever(top_k_results=2, doc_content_chars_max=1500, wiki_client=None)\n",
    "\n",
    "# Function to combine results from multiple retrievers\n",
    "def multi_retriever(query):\n",
    "    \"\"\"Retrieve from multiple sources and combine results.\"\"\"\n",
    "    arxiv_docs = arxiv_retriever.invoke(query)\n",
    "    wiki_docs = wiki_retriever.invoke(query)\n",
    "    \n",
    "    # Combine and format\n",
    "    all_docs = []\n",
    "    \n",
    "    if arxiv_docs:\n",
    "        all_docs.append(\"=== Academic Papers (ArXiv) ===\")\n",
    "        all_docs.extend([doc.page_content[:500] for doc in arxiv_docs])\n",
    "    \n",
    "    if wiki_docs:\n",
    "        all_docs.append(\"\\n=== General Knowledge (Wikipedia) ===\")\n",
    "        all_docs.extend([doc.page_content[:500] for doc in wiki_docs])\n",
    "    \n",
    "    return \"\\n\\n\".join(all_docs)   # This ( all_docs) is having the combined Context \n",
    "\n",
    "# Create multi-source RAG chain\n",
    "multi_source_template = \"\"\"Answer the question using information from multiple sources below:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Provide a comprehensive answer that synthesizes information from both academic and general sources:\"\"\"\n",
    "\n",
    "multi_prompt = ChatPromptTemplate.from_template(multi_source_template)  # This is just a variable name\n",
    "\n",
    "multi_rag_chain = (\n",
    "    {\"context\": multi_retriever, \"question\": RunnablePassthrough()}\n",
    "    | multi_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Ask a question\n",
    "question = \"What are transformers in machine learning?\"\n",
    "answer = multi_rag_chain.invoke(question)\n",
    "\n",
    "print(f\"Question: {question}\\n\")\n",
    "print(f\"Answer (from multiple sources):\\n{answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéì INTERMEDIATE: Real-Time RAG with TavilySearchAPIRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What are the latest developments in AI regulation?\n",
      "\n",
      "Answer (from real-time web search):\n",
      "As of the latest information available, there have been significant developments in AI regulation globally, with a focus on ensuring ethical and responsible use of artificial intelligence technologies. One of the most notable developments is the regulation passed by the European Union, which will come into effect on August 1, 2024. This regulation aims to establish harmonized rules for AI across all 27 EU member states, with the goal of promoting trust and transparency in AI systems.\n",
      "\n",
      "The EU regulation on AI is expected to address various aspects of AI development and deployment, including data privacy, algorithmic transparency, and accountability. It will set out clear guidelines for the use of AI in different sectors, such as healthcare, finance, and transportation, to ensure that AI systems are developed and used in a way that respects fundamental rights and values.\n",
      "\n",
      "In addition to the EU regulation, several individual states have also passed laws regulating the development and use of AI systems and technologies. These laws, enacted between 2019 and 2025, aim to address specific concerns related to AI, such as bias and discrimination, data protection, and accountability. By implementing these laws, states are taking proactive steps to ensure that AI technologies are developed and deployed in a manner that is ethical and responsible.\n",
      "\n",
      "For example, some states have introduced laws that require companies to disclose the use of AI in decision-making processes, particularly in areas such as hiring, lending, and criminal justice. These laws aim to increase transparency and accountability in AI systems, allowing individuals to understand how AI is being used and to challenge decisions that may be biased or discriminatory.\n",
      "\n",
      "Furthermore, there have been efforts to regulate the use of AI in consumer protection, particularly in the retail sector. The new law mentioned in the latest information seeks to prevent retailers from using AI and personal data to charge consumers unfairly. By regulating the use of AI in pricing and marketing strategies, states are working to protect consumers from potential exploitation and ensure fair competition in the marketplace.\n",
      "\n",
      "Overall, the latest developments in AI regulation reflect a growing recognition of the need to establish clear guidelines and standards for the development and use of AI technologies. By enacting laws that promote transparency, accountability, and ethical use of AI, states are taking important steps to address the challenges and risks associated with AI while harnessing its potential benefits for society.\n",
      "\n",
      "Sources:\n",
      "1. European Commission - Proposal for a Regulation on AI: https://ec.europa.eu/digital-single-market/en/news/proposal-regulation-laying-down-harmonised-rules-artificial-intelligence-artificial-intelligence\n",
      "2. National Conference of State Legislatures - Artificial Intelligence Legislation: https://www.ncsl.org/research/telecommunications-and-information-technology/artificial-intelligence-legislation.aspx\n"
     ]
    }
   ],
   "source": [
    "# Build a RAG chain that uses real-time web search\n",
    "tavily_retriever = TavilySearchAPIRetriever(k=3)\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "# Create prompt for real-time information\n",
    "realtime_template = \"\"\"Based on the latest information from the web:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Provide an up-to-date answer having atleast 500 words with source attribution:\"\"\"\n",
    "\n",
    "realtime_prompt = ChatPromptTemplate.from_template(realtime_template)\n",
    "\n",
    "# Build real-time RAG chain\n",
    "realtime_rag_chain = (\n",
    "    {\"context\": tavily_retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | realtime_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Ask a current events question\n",
    "question = \"What are the latest developments in AI regulation?\"\n",
    "answer = realtime_rag_chain.invoke(question)\n",
    "\n",
    "print(f\"Question: {question}\\n\")\n",
    "print(f\"Answer (from real-time web search):\\n{answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='comparison'></a>\n",
    "## 7. Comparison & Use Cases üìä\n",
    "\n",
    "### Retriever Comparison Table\n",
    "\n",
    "| Feature | ArxivRetriever | WikipediaRetriever | TavilySearchAPIRetriever |\n",
    "|---------|----------------|-------------------|-------------------------|\n",
    "| **Data Source** | Academic papers (arxiv.org) | Wikipedia articles | Real-time web search |\n",
    "| **API Key Required** | ‚ùå No | ‚ùå No | ‚úÖ Yes (free tier) |\n",
    "| **Data Freshness** | Recent research | Regularly updated | Real-time |\n",
    "| **Best For** | Academic research, ML papers | General knowledge, definitions | Current events, news |\n",
    "| **Content Type** | Research papers, abstracts | Encyclopedia articles | Web pages, news |\n",
    "| **Default Results** | 3 papers | 3 articles | 5 results |\n",
    "| **Multilingual** | ‚ùå No | ‚úÖ Yes (300+ languages) | ‚úÖ Yes |\n",
    "| **Metadata** | Title, Authors, Published date | Title, Summary, URL | Source URL, Score |\n",
    "| **Rate Limits** | Moderate | Moderate | API-dependent |\n",
    "| **Cost** | üÜì Free | üÜì Free | üÜì Free tier + paid |\n",
    "\n",
    "---\n",
    "\n",
    "### When to Use Each Retriever\n",
    "\n",
    "#### ‚úÖ Use **ArxivRetriever** when:\n",
    "- You need peer-reviewed academic research\n",
    "- You're building an AI/ML research assistant\n",
    "- You want the latest scientific papers\n",
    "- You need citations and author information\n",
    "\n",
    "#### ‚úÖ Use **WikipediaRetriever** when:\n",
    "- You need general knowledge and definitions\n",
    "- You want historical or biographical information\n",
    "- You're building an educational chatbot\n",
    "- You need multilingual support\n",
    "- You want reliable, community-edited content\n",
    "\n",
    "#### ‚úÖ Use **TavilySearchAPIRetriever** when:\n",
    "- You need real-time, up-to-date information\n",
    "- You're answering current events questions\n",
    "- You want to search the broader internet\n",
    "- You need to filter by specific domains\n",
    "- Your use case requires the latest data\n",
    "\n",
    "---\n",
    "\n",
    "### Combining Retrievers (Hybrid Approach)\n",
    "\n",
    "For the most comprehensive RAG system:\n",
    "\n",
    "```python\n",
    "# Pseudo-code for hybrid retrieval\n",
    "if query_type == \"academic\":\n",
    "    use ArxivRetriever\n",
    "elif query_type == \"general_knowledge\":\n",
    "    use WikipediaRetriever\n",
    "elif query_type == \"current_events\":\n",
    "    use TavilySearchAPIRetriever\n",
    "else:\n",
    "    # Use multiple retrievers and combine results\n",
    "    combine(ArxivRetriever, WikipediaRetriever, TavilySearchAPIRetriever)\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='best-practices'></a>\n",
    "## 8. Best Practices üí°\n",
    "\n",
    "### General Best Practices\n",
    "\n",
    "#### 1. **Handle Errors Gracefully**\n",
    "\n",
    "```python\n",
    "try:\n",
    "    docs = retriever.invoke(query)\n",
    "except Exception as e:\n",
    "    print(f\"Error retrieving documents: {e}\")\n",
    "    docs = []  # Fallback to empty list\n",
    "```\n",
    "\n",
    "#### 2. **Set Appropriate Limits**\n",
    "\n",
    "```python\n",
    "# Don't retrieve too many documents (costs, latency)\n",
    "arxiv_retriever = ArxivRetriever(load_max_docs=3)  # ‚úÖ Good\n",
    "arxiv_retriever = ArxivRetriever(load_max_docs=100)  # ‚ùå Too many\n",
    "```\n",
    "\n",
    "#### 3. **Cache Results for Repeated Queries**\n",
    "\n",
    "```python\n",
    "# Use a simple cache to avoid redundant API calls\n",
    "from functools import lru_cache\n",
    "\n",
    "@lru_cache(maxsize=100)\n",
    "def cached_search(query: str):\n",
    "    return retriever.invoke(query)\n",
    "```\n",
    "\n",
    "#### 4. **Verify Source Attribution**\n",
    "\n",
    "```python\n",
    "# Always include sources in your responses\n",
    "for doc in docs:\n",
    "    print(f\"Source: {doc.metadata.get('source', 'N/A')}\")\n",
    "```\n",
    "\n",
    "#### 5. **Combine with Vector Store Retrievers**\n",
    "\n",
    "```python\n",
    "# Use external retrievers for general knowledge\n",
    "# Use vector stores for your proprietary data\n",
    "def hybrid_retrieve(query):\n",
    "    external_docs = wiki_retriever.invoke(query)\n",
    "    internal_docs = vector_store.similarity_search(query)\n",
    "    return external_docs + internal_docs\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Retriever-Specific Best Practices\n",
    "\n",
    "#### ArxivRetriever:\n",
    "- ‚úÖ Use specific search terms (e.g., \"BERT transformers\" vs \"AI\")\n",
    "- ‚úÖ Limit results to 3-5 papers for LLM context\n",
    "- ‚úÖ Extract metadata for citations\n",
    "- ‚ùå Don't use for non-academic queries\n",
    "\n",
    "#### WikipediaRetriever:\n",
    "- ‚úÖ Use for general knowledge, not specialized topics\n",
    "- ‚úÖ Set `doc_content_chars_max` to avoid huge documents\n",
    "- ‚úÖ Verify information for critical use cases\n",
    "- ‚ùå Don't rely on Wikipedia for real-time information\n",
    "\n",
    "#### TavilySearchAPIRetriever:\n",
    "- ‚úÖ Monitor API usage (rate limits, costs)\n",
    "- ‚úÖ Use for time-sensitive queries\n",
    "- ‚úÖ Filter by domain for specific sources\n",
    "- ‚ùå Don't use for queries that don't need real-time data\n",
    "\n",
    "---\n",
    "\n",
    "### Performance Tips\n",
    "\n",
    "1. **Use `.batch()` for multiple queries**\n",
    "   ```python\n",
    "   # ‚úÖ Efficient\n",
    "   results = retriever.batch([q1, q2, q3])\n",
    "   \n",
    "   # ‚ùå Inefficient\n",
    "   results = [retriever.invoke(q) for q in [q1, q2, q3]]\n",
    "   ```\n",
    "\n",
    "2. **Limit document length for LLM context**\n",
    "   ```python\n",
    "   # Truncate long documents to fit LLM context window\n",
    "   docs = [Document(page_content=doc.page_content[:2000], metadata=doc.metadata) \n",
    "           for doc in raw_docs]\n",
    "   ```\n",
    "\n",
    "3. **Use async methods for concurrent retrieval** (if supported)\n",
    "   ```python\n",
    "   # For async-compatible retrievers\n",
    "   import asyncio\n",
    "   docs = await retriever.ainvoke(query)\n",
    "   ```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='summary'></a>\n",
    "## 9. Summary & Exercises üìù\n",
    "\n",
    "### üéØ What You Learned\n",
    "\n",
    "In this notebook, you learned:\n",
    "\n",
    "‚úÖ **External Index Retrievers** - Search over external data sources (internet, databases)\n",
    "\n",
    "‚úÖ **ArxivRetriever** - Retrieve academic papers from arxiv.org\n",
    "   - Use cases: Research, ML papers, citations\n",
    "   - Methods: `.invoke()`, `.batch()`\n",
    "   - Metadata: Title, Authors, Published date\n",
    "\n",
    "‚úÖ **WikipediaRetriever** - Access Wikipedia articles\n",
    "   - Use cases: General knowledge, definitions, history\n",
    "   - Features: Multilingual support, customizable length\n",
    "   - Metadata: Title, Summary, Source URL\n",
    "\n",
    "‚úÖ **TavilySearchAPIRetriever** - Real-time web search\n",
    "   - Use cases: Current events, news, real-time data\n",
    "   - Features: Domain filtering, search depth control\n",
    "   - Metadata: Source URL, Relevance score\n",
    "\n",
    "‚úÖ **RAG Integration** - Combined external retrievers with LLMs\n",
    "   - Built simple QA chains\n",
    "   - Created multi-source RAG systems\n",
    "   - Implemented real-time information retrieval\n",
    "\n",
    "‚úÖ **Best Practices** - Error handling, caching, source attribution\n",
    "\n",
    "---\n",
    "\n",
    "### üí™ Practice Exercises\n",
    "\n",
    "#### Exercise 1: Academic Research Assistant (üî∞ Beginner)\n",
    "Create a RAG chain that:\n",
    "- Uses `ArxivRetriever` to find papers on \"deep learning\"\n",
    "- Extracts the top 3 paper titles and authors\n",
    "- Summarizes each paper's abstract using an LLM\n",
    "\n",
    "#### Exercise 2: Wikipedia Fact Checker (üî∞ Beginner)\n",
    "Build a system that:\n",
    "- Takes a statement as input (e.g., \"Python was created in 1991\")\n",
    "- Uses `WikipediaRetriever` to search for relevant articles\n",
    "- Uses an LLM to verify if the statement is accurate\n",
    "\n",
    "#### Exercise 3: Multi-Source News Aggregator (üéì Intermediate)\n",
    "Create a RAG chain that:\n",
    "- Uses `TavilySearchAPIRetriever` to get latest AI news\n",
    "- Uses `WikipediaRetriever` to get background on AI topics\n",
    "- Combines both sources to provide a comprehensive news summary\n",
    "\n",
    "#### Exercise 4: Hybrid Retrieval System (üéì Intermediate)\n",
    "Build a system that:\n",
    "- Classifies queries into \"academic\", \"general\", or \"current_events\"\n",
    "- Routes to the appropriate retriever based on query type\n",
    "- Returns results from the most relevant source\n",
    "\n",
    "#### Exercise 5: Multilingual Knowledge Base (üöÄ Advanced)\n",
    "Create a system that:\n",
    "- Detects the language of the user's query\n",
    "- Uses `WikipediaRetriever` with the appropriate language setting\n",
    "- Returns answers in the user's language\n",
    "\n",
    "---\n",
    "\n",
    "### üîó Next Steps\n",
    "\n",
    "- **Notebook 09**: Advanced Retrieval Techniques (Hybrid Search, Re-ranking)\n",
    "- **Notebook 10**: Production RAG Systems (Caching, Monitoring, Scaling)\n",
    "- **LangChain Documentation**: https://python.langchain.com/docs/integrations/retrievers/\n",
    "\n",
    "---\n",
    "\n",
    "### üìö Additional Resources\n",
    "\n",
    "- **ArXiv**: https://arxiv.org/\n",
    "- **Wikipedia API**: https://www.mediawiki.org/wiki/API:Main_page\n",
    "- **Tavily API**: https://tavily.com/\n",
    "- **LangChain Retrievers**: https://python.langchain.com/docs/modules/data_connection/retrievers/\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations!** üéâ You've mastered external index retrievers in LangChain!\n",
    "\n",
    "You can now build RAG systems that access:\n",
    "- üìÑ Academic research (ArXiv)\n",
    "- üìñ General knowledge (Wikipedia)\n",
    "- üåê Real-time web data (Tavily)\n",
    "\n",
    "Keep experimenting and building amazing AI applications! üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lcrag_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
