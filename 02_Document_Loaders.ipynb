{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìÅ Notebook 02: Document Loaders\n",
    "\n",
    "**LangChain 1.0.5+ | Mixed Level Class**\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "1. Load documents from **PDF files** using PyPDFLoader\n",
    "2. Load structured data from **CSV files**\n",
    "3. Load JSON data from **API responses** or files\n",
    "4. Scrape and load content from **web pages** (HTML)\n",
    "5. Load **text files** and **markdown files**\n",
    "6. **Batch process** multiple files using DirectoryLoader\n",
    "7. Understand Document object structure\n",
    "\n",
    "---\n",
    "\n",
    "## üìñ Table of Contents\n",
    "\n",
    "1. [Why Document Loaders?](#why-loaders)\n",
    "2. [Document Object Structure](#document-structure)\n",
    "3. [Loading PDF Files](#pdf-loading)\n",
    "4. [Loading CSV Files](#csv-loading)\n",
    "5. [Loading JSON Files](#json-loading)\n",
    "6. [Loading Web Pages (HTML)](#html-loading)\n",
    "7. [Loading Text and Markdown Files](#text-loading)\n",
    "8. [Batch Loading with DirectoryLoader](#batch-loading)\n",
    "9. [Comparison Table](#comparison)\n",
    "10. [Best Practices](#best-practices)\n",
    "11. [Summary & Exercises](#summary)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"why-loaders\"></a>\n",
    "## 1. Why Document Loaders? ü§î\n",
    "\n",
    "### üî∞ BEGINNER\n",
    "\n",
    "**Document Loaders** are tools that help you convert files (PDFs, CSVs, web pages, etc.) into **Document objects** that LangChain can work with.\n",
    "\n",
    "Think of them as **translators**:\n",
    "- **Input**: Files in various formats (PDF, CSV, JSON, HTML)\n",
    "- **Output**: Standardized Document objects with text content and metadata\n",
    "\n",
    "### Why is this important?\n",
    "\n",
    "Every RAG application needs to:\n",
    "1. üì• **Load** data from various sources\n",
    "2. üîÑ **Convert** it to a standard format\n",
    "3. üìä **Extract** metadata (source, page number, etc.)\n",
    "4. üéØ **Prepare** it for embedding and retrieval\n",
    "\n",
    "Document Loaders handle all of this automatically!\n",
    "\n",
    "### üéì INTERMEDIATE\n",
    "\n",
    "All document loaders in LangChain implement the same interface:\n",
    "- `.load()`: Load all documents at once (returns list[Document])\n",
    "- `.lazy_load()`: Load documents one at a time (generator, memory efficient)\n",
    "\n",
    "This consistency makes it easy to switch between different data sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Environment loaded\n",
      "Current directory: d:\\AiCode\\LcRAGBtCmp\\simple-rag-langchain\n",
      "Sample data directory exists: True\n"
     ]
    }
   ],
   "source": [
    "# Setup: Import required libraries\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Verify setup\n",
    "print(\"‚úÖ Environment loaded\")\n",
    "print(f\"Current directory: {os.getcwd()}\")\n",
    "print(f\"Sample data directory exists: {Path('sample_data').exists()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"document-structure\"></a>\n",
    "## 2. Document Object Structure üìÑ\n",
    "\n",
    "### üî∞ BEGINNER\n",
    "\n",
    "Every Document has two main parts:\n",
    "1. **page_content**: The actual text (string)\n",
    "2. **metadata**: Information about the document (dictionary)\n",
    "\n",
    "Think of it like a book:\n",
    "- **page_content** = The story\n",
    "- **metadata** = The cover information (title, author, page number, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Document Structure:\n",
      "\n",
      "Type: <class 'langchain_core.documents.base.Document'>\n",
      "\n",
      "Content (first 100 chars): This is the actual content of the document. It contains the text we want to process....\n",
      "\n",
      "Metadata: {'source': 'example.pdf', 'page': 1, 'author': 'John Doe', 'date': '2025-01-15'}\n",
      "\n",
      "Source: example.pdf\n",
      "Page Number: 1\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "# Create a sample document\n",
    "doc = Document(\n",
    "    page_content=\"This is the actual content of the document. It contains the text we want to process.\",\n",
    "    metadata={\n",
    "        \"source\": \"example.pdf\",\n",
    "        \"page\": 1,\n",
    "        \"author\": \"John Doe\",\n",
    "        \"date\": \"2025-01-15\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Inspect the document\n",
    "print(\"üìÑ Document Structure:\")\n",
    "print(f\"\\nType: {type(doc)}\")\n",
    "print(f\"\\nContent (first 100 chars): {doc.page_content[:100]}...\")\n",
    "print(f\"\\nMetadata: {doc.metadata}\")\n",
    "print(f\"\\nSource: {doc.metadata['source']}\")\n",
    "print(f\"Page Number: {doc.metadata['page']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"pdf-loading\"></a>\n",
    "## 3. Loading PDF Files üìï\n",
    "\n",
    "### üî∞ BEGINNER\n",
    "\n",
    "**PyPDFLoader** is used to load PDF files. It:\n",
    "- Extracts text from each page\n",
    "- Creates one Document per page\n",
    "- Automatically adds source and page number to metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: Loading a Single PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading PDF: ./pdfs/attention.pdf\n",
      "‚è≥ This may take a moment...\n",
      "\n",
      "‚úÖ Loaded 15 pages\n",
      "\n",
      "üìÑ First Page:\n",
      "   Content (first 200 chars): Provided proper attribution is provided, Google hereby grants permission to\n",
      "reproduce the tables and figures in this paper solely for use in journalistic or\n",
      "scholarly works.\n",
      "Attention Is All You Need\n",
      "...\n",
      "\n",
      "   Metadata: {'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': './pdfs/attention.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1'}\n",
      "\n",
      "üìÑ Last Page (page 15):\n",
      "   Content (first 200 chars): Input-Input Layer5\n",
      "The\n",
      "Law\n",
      "will\n",
      "never\n",
      "be\n",
      "perfect\n",
      ",\n",
      "but\n",
      "its\n",
      "application\n",
      "should\n",
      "be\n",
      "just\n",
      "-\n",
      "this\n",
      "is\n",
      "what\n",
      "we\n",
      "are\n",
      "missing\n",
      ",\n",
      "in\n",
      "my\n",
      "opinion\n",
      ".\n",
      "<EOS>\n",
      "<pad>\n",
      "The\n",
      "Law\n",
      "will\n",
      "never\n",
      "be\n",
      "perfect\n",
      ",\n",
      "but\n",
      "its\n",
      "application\n",
      "sh...\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "# Load the \"Attention is All You Need\" paper (if it exists)\n",
    "pdf_path = \"./pdfs/attention.pdf\"\n",
    "\n",
    "if Path(pdf_path).exists():\n",
    "    print(f\"Loading PDF: {pdf_path}\")\n",
    "    print(\"‚è≥ This may take a moment...\\n\")\n",
    "    \n",
    "    # Create loader\n",
    "    loader = PyPDFLoader(pdf_path)\n",
    "    \n",
    "    # Load all pages\n",
    "    documents = loader.load()\n",
    "    \n",
    "    print(f\"‚úÖ Loaded {len(documents)} pages\\n\")\n",
    "    \n",
    "    # Inspect first page\n",
    "    print(\"üìÑ First Page:\")\n",
    "    print(f\"   Content (first 200 chars): {documents[0].page_content[:200]}...\")\n",
    "    print(f\"\\n   Metadata: {documents[0].metadata}\")\n",
    "    \n",
    "    # Inspect last page\n",
    "    print(f\"\\nüìÑ Last Page (page {len(documents)}):\")\n",
    "    print(f\"   Content (first 200 chars): {documents[-1].page_content[:200]}...\")\n",
    "    \n",
    "else:\n",
    "    print(f\"‚ùå PDF not found: {pdf_path}\")\n",
    "    print(\"   Make sure the file exists in the project root\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéì INTERMEDIATE: Lazy Loading for Large PDFs\n",
    "\n",
    "For very large PDFs, use `.lazy_load()` to process one page at a time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Lazy loading pages (memory efficient):\n",
      "\n",
      "Page 1:\n",
      "  Length: 2859 characters\n",
      "  Preview: Provided proper attribution is provided, Google hereby grants permission to\n",
      "reproduce the tables and...\n",
      "\n",
      "Page 2:\n",
      "  Length: 4257 characters\n",
      "  Preview: 1 Introduction\n",
      "Recurrent neural networks, long short-term memory [13] and gated recurrent [7] neural...\n",
      "\n",
      "Page 3:\n",
      "  Length: 1826 characters\n",
      "  Preview: Figure 1: The Transformer - model architecture.\n",
      "The Transformer follows this overall architecture us...\n",
      "\n",
      "Page 4:\n",
      "  Length: 2505 characters\n",
      "  Preview: Scaled Dot-Product Attention\n",
      " Multi-Head Attention\n",
      "Figure 2: (left) Scaled Dot-Product Attention. (r...\n",
      "\n",
      "Page 5:\n",
      "  Length: 3188 characters\n",
      "  Preview: output values. These are concatenated and once again projected, resulting in the final values, as\n",
      "de...\n",
      "\n",
      "üí° Tip: Use lazy_load() for PDFs > 100 pages to save memory\n"
     ]
    }
   ],
   "source": [
    "# Lazy loading example\n",
    "if Path(pdf_path).exists():\n",
    "    loader = PyPDFLoader(pdf_path)\n",
    "    \n",
    "    print(\"üîÑ Lazy loading pages (memory efficient):\")\n",
    "    \n",
    "    # Process first 3 pages only\n",
    "    for i, page in enumerate(loader.lazy_load()):\n",
    "        if i >= 5:  # Only process first 3 pages for demo\n",
    "            break\n",
    "        \n",
    "        print(f\"\\nPage {i+1}:\")\n",
    "        print(f\"  Length: {len(page.page_content)} characters\")\n",
    "        print(f\"  Preview: {page.page_content[:100]}...\")\n",
    "    \n",
    "    print(\"\\nüí° Tip: Use lazy_load() for PDFs > 100 pages to save memory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Loading Multiple PDFs from a Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loading PDFs from: pdfs/\n",
      "\n",
      "Found 3 PDF files:\n",
      "  - attention.pdf\n",
      "    ‚úÖ Loaded 15 pages\n",
      "  - rag.pdf\n",
      "    ‚úÖ Loaded 19 pages\n",
      "  - ragsurvey.pdf\n",
      "    ‚úÖ Loaded 21 pages\n",
      "\n",
      "üìä Total: 55 pages from 3 PDFs\n",
      "\n",
      "Sources:\n",
      "  - ragsurvey.pdf\n",
      "  - attention.pdf\n",
      "  - rag.pdf\n"
     ]
    }
   ],
   "source": [
    "# Load all PDFs from the pdfs/ directory\n",
    "pdf_directory = \"pdfs\"\n",
    "\n",
    "if Path(pdf_directory).exists():\n",
    "    print(f\"üìÇ Loading PDFs from: {pdf_directory}/\\n\")\n",
    "    \n",
    "    all_documents = []\n",
    "    \n",
    "    # Find all PDF files\n",
    "    pdf_files = list(Path(pdf_directory).glob(\"*.pdf\"))\n",
    "    print(f\"Found {len(pdf_files)} PDF files:\")\n",
    "    \n",
    "    for pdf_file in pdf_files:\n",
    "        print(f\"  - {pdf_file.name}\")\n",
    "        \n",
    "        # Load each PDF\n",
    "        loader = PyPDFLoader(str(pdf_file))\n",
    "        docs = loader.load()\n",
    "        all_documents.extend(docs)\n",
    "        \n",
    "        print(f\"    ‚úÖ Loaded {len(docs)} pages\")\n",
    "    \n",
    "    print(f\"\\nüìä Total: {len(all_documents)} pages from {len(pdf_files)} PDFs\")\n",
    "    \n",
    "    # Show unique sources\n",
    "    sources = set(doc.metadata['source'] for doc in all_documents)\n",
    "    print(f\"\\nSources:\")\n",
    "    for source in sources:\n",
    "        print(f\"  - {Path(source).name}\")\n",
    "        \n",
    "else:\n",
    "    print(f\"‚ùå Directory not found: {pdf_directory}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"csv-loading\"></a>\n",
    "## 4. Loading CSV Files üìä\n",
    "\n",
    "### üî∞ BEGINNER\n",
    "\n",
    "**CSVLoader** converts each row of a CSV file into a separate Document.\n",
    "\n",
    "**Use cases:**\n",
    "- Product catalogs\n",
    "- FAQ databases\n",
    "- Customer records\n",
    "- Any tabular data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CSV: sample_data/products.csv\n",
      "\n",
      "‚úÖ Loaded 15 products\n",
      "\n",
      "======================================================================\n",
      "Product 1:\n",
      "======================================================================\n",
      "product_id: 1\n",
      "product_name: Laptop Pro 15\n",
      "category: Electronics\n",
      "description: High-performance laptop with 15-inch display, Intel i7 processor, 16GB RAM, and 512GB SSD. Perfect for professional work and gaming.\n",
      "price: 1299.99\n",
      "stock: 45\n",
      "\n",
      "Source: Laptop Pro 15\n",
      "Row: 0\n",
      "\n",
      "======================================================================\n",
      "Product 2:\n",
      "======================================================================\n",
      "product_id: 2\n",
      "product_name: Wireless Mouse\n",
      "category: Accessories\n",
      "description: Ergonomic wireless mouse with 6 programmable buttons, 2400 DPI optical sensor, and long battery life. Compatible with Windows and Mac.\n",
      "price: 29.99\n",
      "stock: 150\n",
      "\n",
      "Source: Wireless Mouse\n",
      "Row: 1\n",
      "\n",
      "======================================================================\n",
      "Product 3:\n",
      "======================================================================\n",
      "product_id: 3\n",
      "product_name: USB-C Hub\n",
      "category: Accessories\n",
      "description: 7-in-1 USB-C hub with HDMI, USB 3.0 ports, SD card reader, and USB-C power delivery. Ideal for laptops and tablets.\n",
      "price: 49.99\n",
      "stock: 80\n",
      "\n",
      "Source: USB-C Hub\n",
      "Row: 2\n",
      "\n",
      "... and 12 more products\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import CSVLoader\n",
    "\n",
    "# Load the products CSV\n",
    "csv_path = \"sample_data/products.csv\"\n",
    "\n",
    "if Path(csv_path).exists():\n",
    "    print(f\"Loading CSV: {csv_path}\\n\")\n",
    "    \n",
    "    # Create loader\n",
    "    loader = CSVLoader(\n",
    "        file_path=csv_path,\n",
    "        source_column=\"product_name\"  # Which column to use as source in metadata\n",
    "    )\n",
    "    \n",
    "    # Load all rows\n",
    "    documents = loader.load()\n",
    "    \n",
    "    print(f\"‚úÖ Loaded {len(documents)} products\\n\")\n",
    "    \n",
    "    # Inspect first 3 products\n",
    "    for i, doc in enumerate(documents[:3], 1):\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"Product {i}:\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(doc.page_content)\n",
    "        print(f\"\\nSource: {doc.metadata['source']}\")\n",
    "        print(f\"Row: {doc.metadata.get('row', 'N/A')}\")\n",
    "        print()\n",
    "    \n",
    "    print(f\"... and {len(documents) - 3} more products\")\n",
    "    \n",
    "else:\n",
    "    print(f\"‚ùå CSV not found: {csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéì INTERMEDIATE: Custom CSV Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä CSV with custom configuration:\n",
      "\n",
      "First document source: 1\n",
      "Content preview:\n",
      "product_id: 1\n",
      "product_name: Laptop Pro 15\n",
      "category: Electronics\n",
      "description: High-performance laptop with 15-inch display, Intel i7 processor, 16GB RAM, and 512GB SSD. Perfect for professional work an...\n"
     ]
    }
   ],
   "source": [
    "if Path(csv_path).exists():\n",
    "    # Advanced CSV loading with custom configuration\n",
    "    loader = CSVLoader(\n",
    "        file_path=csv_path,\n",
    "        csv_args={\n",
    "            'delimiter': ',',\n",
    "            'quotechar': '\"',\n",
    "            'fieldnames': None,  # Use first row as headers\n",
    "        },\n",
    "        source_column=\"product_id\"  # Use product_id as source\n",
    "    )\n",
    "    \n",
    "    docs = loader.load()\n",
    "    \n",
    "    # Show how metadata is different\n",
    "    print(\"üìä CSV with custom configuration:\\n\")\n",
    "    print(f\"First document source: {docs[0].metadata['source']}\")\n",
    "    print(f\"Content preview:\\n{docs[0].page_content[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"json-loading\"></a>\n",
    "## 5. Loading JSON Files üîß\n",
    "\n",
    "### üî∞ BEGINNER\n",
    "\n",
    "**JSONLoader** extracts data from JSON files using **jq** syntax (a query language for JSON).\n",
    "\n",
    "**Common use cases:**\n",
    "- API responses\n",
    "- Configuration files\n",
    "- Structured data exports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jq in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (1.10.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install jq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading JSON: ./sample_data/api_response.json\n",
      "\n",
      "‚úÖ Loaded 5 articles\n",
      "\n",
      "üì∞ First Article:\n",
      "Content:\n",
      "{\"id\": \"article_001\", \"title\": \"Introduction to Retrieval-Augmented Generation (RAG)\", \"author\": \"Dr. Sarah Chen\", \"published_date\": \"2025-01-10\", \"category\": \"Machine Learning\", \"tags\": [\"RAG\", \"LLM\", \"NLP\", \"AI\"], \"summary\": \"Retrieval-Augmented Generation (RAG) is a powerful technique that combines information retrieval with large language models to generate more accurate and contextual responses.\", \"content\": \"RAG systems work by first retrieving relevant documents from a knowledge base, then using those documents as context for a language model to generate responses. This approach significantly reduces hallucinations and provides more factual, grounded outputs. The architecture typically consists of three main components: a document store, an embedding model for semantic search, and a language model for generation.\", \"reading_time\": \"5 minutes\", \"views\": 15420, \"likes\": 892}\n",
      "\n",
      "Metadata: {'source': 'D:\\\\AiCode\\\\LcRAGBtCmp\\\\simple-rag-langchain\\\\sample_data\\\\api_response.json', 'seq_num': 1}\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import JSONLoader\n",
    "from pathlib import Path\n",
    "\n",
    "# Load the API response JSON\n",
    "json_path = \"./sample_data/api_response.json\"\n",
    "\n",
    "if Path(json_path).exists():\n",
    "    print(f\"Loading JSON: {json_path}\\n\")\n",
    "    \n",
    "    # Create loader\n",
    "    # jq_schema tells us where to find the content in the JSON\n",
    "    # .articles[] means: get all items from the 'articles' array\n",
    "    loader = JSONLoader(\n",
    "        file_path=json_path,\n",
    "        jq_schema=\".articles[]\",  # Extract each article\n",
    "        text_content=False  # Return full JSON for each article\n",
    "    )\n",
    "    \n",
    "    # Load articles\n",
    "    documents = loader.load()\n",
    "    \n",
    "    print(f\"‚úÖ Loaded {len(documents)} articles\\n\")\n",
    "    \n",
    "    # Inspect first article\n",
    "    print(\"üì∞ First Article:\")\n",
    "    print(f\"Content:\\n{documents[0].page_content}\\n\")\n",
    "    print(f\"Metadata: {documents[0].metadata}\")\n",
    "    \n",
    "else:\n",
    "    print(f\"‚ùå JSON not found: {json_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéì INTERMEDIATE: Extracting Specific Fields from JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Extracted Article Contents Only:\n",
      "\n",
      "1. RAG systems work by first retrieving relevant documents from a knowledge base, then using those documents as context for a language model to generate ...\n",
      "\n",
      "2. Vector databases like FAISS, Pinecone, and Chroma provide optimized storage and retrieval for embedding vectors. Unlike traditional databases that use...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if Path(json_path).exists():\n",
    "    # Extract only the article content field\n",
    "    loader = JSONLoader(\n",
    "        file_path=json_path,\n",
    "        jq_schema=\".articles[].content\",  # Get only 'content' field\n",
    "        text_content=True  # Treat as plain text\n",
    "    )\n",
    "    \n",
    "    docs = loader.load()\n",
    "    \n",
    "    print(\"üìù Extracted Article Contents Only:\\n\")\n",
    "    for i, doc in enumerate(docs[:2], 1):\n",
    "        print(f\"{i}. {doc.page_content[:150]}...\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üî∞ BEGINNER TIP: Understanding jq Syntax\n",
    "\n",
    "**jq** is like a GPS for JSON:\n",
    "\n",
    "| jq Expression | Meaning |\n",
    "|--------------|----------|\n",
    "| `.` | Root of JSON |\n",
    "| `.articles` | Get the 'articles' field |\n",
    "| `.articles[]` | Get all items in 'articles' array |\n",
    "| `.articles[0]` | Get first item in 'articles' array |\n",
    "| `.articles[].title` | Get 'title' from each article |\n",
    "\n",
    "**Example:**\n",
    "```json\n",
    "{\n",
    "  \"articles\": [\n",
    "    {\"title\": \"Article 1\", \"content\": \"...\"},\n",
    "    {\"title\": \"Article 2\", \"content\": \"...\"}\n",
    "  ]\n",
    "}\n",
    "```\n",
    "- `.articles[]` ‚Üí Returns both articles\n",
    "- `.articles[].title` ‚Üí Returns [\"Article 1\", \"Article 2\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"html-loading\"></a>\n",
    "## 6. Loading Web Pages (HTML) üåê\n",
    "\n",
    "### üî∞ BEGINNER\n",
    "\n",
    "**WebBaseLoader** scrapes web pages and extracts text content.\n",
    "\n",
    "**Important:** Only works with **static HTML**. For JavaScript-rendered sites, you'd need Playwright or Selenium.\n",
    "\n",
    "### Example 1: Loading a Local HTML File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: unstructured in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (0.18.21)\n",
      "Requirement already satisfied: charset-normalizer in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from unstructured) (3.4.4)\n",
      "Requirement already satisfied: filetype in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from unstructured) (1.2.0)\n",
      "Requirement already satisfied: python-magic in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from unstructured) (0.4.27)\n",
      "Requirement already satisfied: lxml in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from unstructured) (6.0.2)\n",
      "Requirement already satisfied: nltk in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from unstructured) (3.9.2)\n",
      "Requirement already satisfied: requests in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from unstructured) (2.32.5)\n",
      "Requirement already satisfied: beautifulsoup4 in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from unstructured) (4.14.2)\n",
      "Requirement already satisfied: emoji in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from unstructured) (2.15.0)\n",
      "Requirement already satisfied: dataclasses-json in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from unstructured) (0.6.7)\n",
      "Requirement already satisfied: python-iso639 in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from unstructured) (2025.11.16)\n",
      "Requirement already satisfied: langdetect in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from unstructured) (1.0.9)\n",
      "Requirement already satisfied: numpy in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from unstructured) (2.3.5)\n",
      "Requirement already satisfied: rapidfuzz in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from unstructured) (3.14.3)\n",
      "Requirement already satisfied: backoff in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from unstructured) (2.2.1)\n",
      "Requirement already satisfied: typing-extensions in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from unstructured) (4.15.0)\n",
      "Requirement already satisfied: unstructured-client in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from unstructured) (0.42.4)\n",
      "Requirement already satisfied: wrapt in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from unstructured) (2.0.1)\n",
      "Requirement already satisfied: tqdm in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from unstructured) (4.67.1)\n",
      "Requirement already satisfied: psutil in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from unstructured) (7.1.3)\n",
      "Requirement already satisfied: python-oxmsg in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from unstructured) (0.0.2)\n",
      "Requirement already satisfied: html5lib in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from unstructured) (1.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from beautifulsoup4->unstructured) (2.8)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from dataclasses-json->unstructured) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from dataclasses-json->unstructured) (0.9.0)\n",
      "Requirement already satisfied: packaging>=17.0 in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->unstructured) (25.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json->unstructured) (1.1.0)\n",
      "Requirement already satisfied: six>=1.9 in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from html5lib->unstructured) (1.17.0)\n",
      "Requirement already satisfied: webencodings in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from html5lib->unstructured) (0.5.1)\n",
      "Requirement already satisfied: click in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from nltk->unstructured) (8.3.1)\n",
      "Requirement already satisfied: joblib in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from nltk->unstructured) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from nltk->unstructured) (2025.11.3)\n",
      "Requirement already satisfied: colorama in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from click->nltk->unstructured) (0.4.6)\n",
      "Requirement already satisfied: olefile in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from python-oxmsg->unstructured) (0.47)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from requests->unstructured) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from requests->unstructured) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from requests->unstructured) (2025.11.12)\n",
      "Requirement already satisfied: aiofiles>=24.1.0 in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from unstructured-client->unstructured) (25.1.0)\n",
      "Requirement already satisfied: cryptography>=3.1 in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from unstructured-client->unstructured) (46.0.3)\n",
      "Requirement already satisfied: httpcore>=1.0.9 in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from unstructured-client->unstructured) (1.0.9)\n",
      "Requirement already satisfied: httpx>=0.27.0 in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from unstructured-client->unstructured) (0.28.1)\n",
      "Requirement already satisfied: pydantic>=2.11.2 in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from unstructured-client->unstructured) (2.12.4)\n",
      "Requirement already satisfied: pypdf>=6.2.0 in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from unstructured-client->unstructured) (6.3.0)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from unstructured-client->unstructured) (1.0.0)\n",
      "Requirement already satisfied: cffi>=2.0.0 in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from cryptography>=3.1->unstructured-client->unstructured) (2.0.0)\n",
      "Requirement already satisfied: pycparser in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from cffi>=2.0.0->cryptography>=3.1->unstructured-client->unstructured) (2.23)\n",
      "Requirement already satisfied: h11>=0.16 in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from httpcore>=1.0.9->unstructured-client->unstructured) (0.16.0)\n",
      "Requirement already satisfied: anyio in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from httpx>=0.27.0->unstructured-client->unstructured) (4.11.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from pydantic>=2.11.2->unstructured-client->unstructured) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from pydantic>=2.11.2->unstructured-client->unstructured) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from pydantic>=2.11.2->unstructured-client->unstructured) (0.4.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from anyio->httpx>=0.27.0->unstructured-client->unstructured) (1.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install unstructured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading HTML: sample_data/blog_post.html\n",
      "\n",
      "‚úÖ Loaded 1 document(s)\n",
      "\n",
      "üìÑ Content length: 7197 characters\n",
      "\n",
      "üìù First 500 characters:\n",
      "Building Intelligent Applications with RAG\n",
      "\n",
      "By Dr. Amanda Foster | January 15, 2025 | 12 min read\n",
      "\n",
      "Introduction\n",
      "\n",
      "In the rapidly evolving landscape of artificial intelligence, Retrieval-Augmented Generation (RAG) has emerged as a game-changing approach for building intelligent applications. Unlike traditional chatbots that rely solely on the knowledge embedded in their training data, RAG systems combine the power of information retrieval with language generation to produce more accurate, contextu...\n",
      "\n",
      "üîç Metadata: {'source': 'sample_data/blog_post.html'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import UnstructuredHTMLLoader\n",
    "\n",
    "# Load our sample blog post\n",
    "html_path = \"sample_data/blog_post.html\"\n",
    "\n",
    "if Path(html_path).exists():\n",
    "    print(f\"Loading HTML: {html_path}\\n\")\n",
    "    \n",
    "    # For local files, we need to use file:// protocol\n",
    "    file_url = f\"file://{Path(html_path).absolute()}\"\n",
    "    \n",
    "    # Create loader\n",
    "    loader = UnstructuredHTMLLoader(html_path)\n",
    "    \n",
    "    # Load the page\n",
    "    documents = loader.load()\n",
    "    \n",
    "    print(f\"‚úÖ Loaded {len(documents)} document(s)\\n\")\n",
    "    \n",
    "    # Inspect content\n",
    "    doc = documents[0]\n",
    "    print(f\"üìÑ Content length: {len(doc.page_content)} characters\")\n",
    "    print(f\"\\nüìù First 500 characters:\\n{doc.page_content[:500]}...\")\n",
    "    print(f\"\\nüîç Metadata: {doc.metadata}\")\n",
    "    \n",
    "else:\n",
    "    print(f\"‚ùå HTML not found: {html_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéì INTERMEDIATE: Loading Multiple URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain_community in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (0.4.1)\n",
      "Requirement already satisfied: langchain-core<2.0.0,>=1.0.1 in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from langchain_community) (1.1.0)\n",
      "Requirement already satisfied: langchain-classic<2.0.0,>=1.0.0 in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from langchain_community) (1.0.0)\n",
      "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from langchain_community) (2.0.44)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.32.5 in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from langchain_community) (2.32.5)\n",
      "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from langchain_community) (6.0.3)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from langchain_community) (3.13.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from langchain_community) (9.1.2)\n",
      "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from langchain_community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from langchain_community) (2.12.0)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.1.125 in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from langchain_community) (0.4.46)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from langchain_community) (0.4.3)\n",
      "Requirement already satisfied: numpy>=2.1.0 in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from langchain_community) (2.3.5)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.22.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain_community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain_community) (0.9.0)\n",
      "Requirement already satisfied: langchain-text-splitters<2.0.0,>=1.0.0 in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from langchain-classic<2.0.0,>=1.0.0->langchain_community) (1.0.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from langchain-classic<2.0.0,>=1.0.0->langchain_community) (2.12.4)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.1->langchain_community) (1.33)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.1->langchain_community) (25.0)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.1->langchain_community) (4.15.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.1->langchain_community) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from langsmith<1.0.0,>=0.1.125->langchain_community) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from langsmith<1.0.0,>=0.1.125->langchain_community) (3.11.4)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from langsmith<1.0.0,>=0.1.125->langchain_community) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from langsmith<1.0.0,>=0.1.125->langchain_community) (0.25.0)\n",
      "Requirement already satisfied: anyio in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain_community) (4.11.0)\n",
      "Requirement already satisfied: certifi in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain_community) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain_community) (1.0.9)\n",
      "Requirement already satisfied: idna in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain_community) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain_community) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-classic<2.0.0,>=1.0.0->langchain_community) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-classic<2.0.0,>=1.0.0->langchain_community) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-classic<2.0.0,>=1.0.0->langchain_community) (0.4.2)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain_community) (1.2.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from requests<3.0.0,>=2.32.5->langchain_community) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from requests<3.0.0,>=2.32.5->langchain_community) (2.3.0)\n",
      "Requirement already satisfied: greenlet>=1 in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain_community) (3.2.4)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain_community) (1.1.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain_community) (1.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2 pages\n",
      "  - https://python.langchain.com/docs/introduction/\n",
      "  - https://python.langchain.com/docs/expression_language/\n",
      "üí° WebBaseLoader Example:\n",
      "\n",
      "To load web pages, use:\n",
      "\n",
      "‚ö†Ô∏è Note: Only works with static HTML (no JavaScript rendering)\n"
     ]
    }
   ],
   "source": [
    "# Example: Load multiple web pages at once\n",
    "# NOTE: This will actually make HTTP requests, so we're using examples\n",
    "\n",
    "%pip install langchain_community\n",
    "\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "#Uncomment to try with real websites:\n",
    "urls = [\n",
    "    \"https://python.langchain.com/docs/introduction/\",\n",
    "    \"https://python.langchain.com/docs/expression_language/\"\n",
    "]\n",
    "\n",
    "loader = WebBaseLoader(urls)\n",
    "docs = loader.load()\n",
    "\n",
    "print(f\"Loaded {len(docs)} pages\")\n",
    "for doc in docs:\n",
    "    print(f\"  - {doc.metadata['source']}\")\n",
    "\n",
    "print(\"üí° WebBaseLoader Example:\")\n",
    "print(\"\\nTo load web pages, use:\")\n",
    "# print(\"\"\"loader = WebBaseLoader([\n",
    "#     \"https://example.com/page1\",\n",
    "#     \"https://example.com/page2\"\n",
    "# ])\"\"\")\n",
    "print(\"\\n‚ö†Ô∏è Note: Only works with static HTML (no JavaScript rendering)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üìÑ LOADED DOCUMENTS CONTENT\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "üìÑ PAGE 1: https://python.langchain.com/docs/introduction/\n",
      "================================================================================\n",
      "\n",
      "üìù Content Preview (first 1000 chars):\n",
      "LangChain overview - Docs by LangChainSkip to main contentüöÄ Share how you're building agents for a chance to win LangChain swag!Docs by LangChain home pageLangChain + LangGraphSearch...‚åòKAsk AIGitHubTry LangSmithTry LangSmithSearch...NavigationLangChain overviewLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonOverviewChangelogGet startedInstallQuickstartPhilosophyCore componentsAgentsModelsMessagesToolsShort-term memoryStreamingStructured outputMiddlewareOverviewBuilt-in middlewareCustom middlewareAdvanced usageGuardrailsRuntimeContext engineeringModel Context Protocol (MCP)Human-in-the-loopMulti-agentRetrievalLong-term memoryAgent developmentLangSmith StudioTestAgent Chat UIDeploy with LangSmithDeploymentObservabilityOn this page Install Create an agent Core benefitsLangChain overviewCopy pageCopy pageLangChain v1.x is now available!For a complete list of changes and instructions on how to upgrade your code, see the release notes and migration guide.If you encoun\n",
      "\n",
      "... [Total length: 3853 characters]\n",
      "\n",
      "üîç Metadata:\n",
      "   source: https://python.langchain.com/docs/introduction/\n",
      "   title: LangChain overview - Docs by LangChain\n",
      "   language: en\n",
      "\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üìÑ PAGE 2: https://python.langchain.com/docs/expression_language/\n",
      "================================================================================\n",
      "\n",
      "üìù Content Preview (first 1000 chars):\n",
      "LangChain overview - Docs by LangChainSkip to main contentüöÄ Share how you're building agents for a chance to win LangChain swag!Docs by LangChain home pageLangChain + LangGraphSearch...‚åòKAsk AIGitHubTry LangSmithTry LangSmithSearch...NavigationLangChain overviewLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonOverviewChangelogGet startedInstallQuickstartPhilosophyCore componentsAgentsModelsMessagesToolsShort-term memoryStreamingStructured outputMiddlewareOverviewBuilt-in middlewareCustom middlewareAdvanced usageGuardrailsRuntimeContext engineeringModel Context Protocol (MCP)Human-in-the-loopMulti-agentRetrievalLong-term memoryAgent developmentLangSmith StudioTestAgent Chat UIDeploy with LangSmithDeploymentObservabilityOn this page Install Create an agent Core benefitsLangChain overviewCopy pageCopy pageLangChain v1.x is now available!For a complete list of changes and instructions on how to upgrade your code, see the release notes and migration guide.If you encoun\n",
      "\n",
      "... [Total length: 3853 characters]\n",
      "\n",
      "üîç Metadata:\n",
      "   source: https://python.langchain.com/docs/expression_language/\n",
      "   title: LangChain overview - Docs by LangChain\n",
      "   language: en\n",
      "\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üìñ FULL CONTENT OF PAGE 1\n",
      "================================================================================\n",
      "LangChain overview - Docs by LangChainSkip to main contentüöÄ Share how you're building agents for a chance to win LangChain swag!Docs by LangChain home pageLangChain + LangGraphSearch...‚åòKAsk AIGitHubTry LangSmithTry LangSmithSearch...NavigationLangChain overviewLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonOverviewChangelogGet startedInstallQuickstartPhilosophyCore componentsAgentsModelsMessagesToolsShort-term memoryStreamingStructured outputMiddlewareOverviewBuilt-in middlewareCustom middlewareAdvanced usageGuardrailsRuntimeContext engineeringModel Context Protocol (MCP)Human-in-the-loopMulti-agentRetrievalLong-term memoryAgent developmentLangSmith StudioTestAgent Chat UIDeploy with LangSmithDeploymentObservabilityOn this page Install Create an agent Core benefitsLangChain overviewCopy pageCopy pageLangChain v1.x is now available!For a complete list of changes and instructions on how to upgrade your code, see the release notes and migration guide.If you encounter any issues or have feedback, please open an issue so we can improve. To view v0.x documentation, go to the archived content.\n",
      "LangChain is the easiest way to start building agents and applications powered by LLMs. With under 10 lines of code, you can connect to OpenAI, Anthropic, Google, and more. LangChain provides a pre-built agent architecture and model integrations to help you get started quickly and seamlessly incorporate LLMs into your agents and applications.\n",
      "We recommend you use LangChain if you want to quickly build agents and autonomous applications. Use LangGraph, our low-level agent orchestration framework and runtime, when you have more advanced needs that require a combination of deterministic and agentic workflows, heavy customization, and carefully controlled latency.\n",
      "LangChain agents are built on top of LangGraph in order to provide durable execution, streaming, human-in-the-loop, persistence, and more. You do not need to know LangGraph for basic LangChain agent usage.\n",
      "‚Äã Install\n",
      "pipuvCopypip install -U langchain\n",
      "# Requires Python 3.10+\n",
      "\n",
      "‚Äã Create an agent\n",
      "Copy# pip install -qU \"langchain[anthropic]\" to call the model\n",
      "\n",
      "from langchain.agents import create_agent\n",
      "\n",
      "def get_weather(city: str) -> str:\n",
      "    \"\"\"Get weather for a given city.\"\"\"\n",
      "    return f\"It's always sunny in {city}!\"\n",
      "\n",
      "agent = create_agent(\n",
      "    model=\"claude-sonnet-4-5-20250929\",\n",
      "    tools=[get_weather],\n",
      "    system_prompt=\"You are a helpful assistant\",\n",
      ")\n",
      "\n",
      "# Run the agent\n",
      "agent.invoke(\n",
      "    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]}\n",
      ")\n",
      "\n",
      "‚Äã Core benefits\n",
      "Standard model interfaceDifferent providers have unique APIs for interacting with models, including the format of responses. LangChain standardizes how you interact with models so that you can seamlessly swap providers and avoid lock-in.Learn moreEasy to use, highly flexible agentLangChain‚Äôs agent abstraction is designed to be easy to get started with, letting you build a simple agent in under 10 lines of code. But it also provides enough flexibility to allow you to do all the context engineering your heart desires.Learn moreBuilt on top of LangGraphLangChain‚Äôs agents are built on top of LangGraph. This allows us to take advantage of LangGraph‚Äôs durable execution, human-in-the-loop support, persistence, and more.Learn moreDebug with LangSmithGain deep visibility into complex agent behavior with visualization tools that trace execution paths, capture state transitions, and provide detailed runtime metrics.Learn more\n",
      "\n",
      "Edit the source of this page on GitHub.\n",
      "Connect these docs programmatically to Claude, VSCode, and more via MCP for real-time answers.Was this page helpful?YesNoChangelogNext‚åòIDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify\n"
     ]
    }
   ],
   "source": [
    "# Print content from both loaded pages\n",
    "print(\"=\"*80)\n",
    "print(\"üìÑ LOADED DOCUMENTS CONTENT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, doc in enumerate(docs, 1):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"üìÑ PAGE {i}: {doc.metadata['source']}\")\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "    # Print first 1000 characters of content\n",
    "    print(f\"\\nüìù Content Preview (first 1000 chars):\")\n",
    "    print(doc.page_content[:1000])\n",
    "    print(f\"\\n... [Total length: {len(doc.page_content)} characters]\")\n",
    "\n",
    "    # Print metadata\n",
    "    print(f\"\\nüîç Metadata:\")\n",
    "    for key, value in doc.metadata.items():\n",
    "        print(f\"   {key}: {value}\")\n",
    "\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Full content of a specific page\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìñ FULL CONTENT OF PAGE 1\")\n",
    "print(\"=\"*80)\n",
    "print(docs[0].page_content)\n",
    "\n",
    "#  Or for a simpler version to just see the content:\n",
    "\n",
    "# Simple version - print both pages\n",
    "# for i, doc in enumerate(docs, 1):\n",
    "#     print(f\"\\n{'='*80}\")\n",
    "#     print(f\"PAGE {i}: {doc.metadata['source']}\")\n",
    "#     print(f\"{'='*80}\\n\")\n",
    "#     print(doc.page_content)\n",
    "#     print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"text-loading\"></a>\n",
    "## 7. Loading Text and Markdown Files üìù\n",
    "\n",
    "### üî∞ BEGINNER\n",
    "\n",
    "For simple text files, use **TextLoader**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading text file: ./sample_data/notes.txt\n",
      "\n",
      "‚úÖ Loaded 1 document\n",
      "\n",
      "üìÑ Content length: 8567 characters\n",
      "\n",
      "üìù First 300 characters:\n",
      "LANGCHAIN STUDY NOTES - RAG IMPLEMENTATION\n",
      "==========================================\n",
      "\n",
      "Date: January 15, 2025\n",
      "Topic: Retrieval-Augmented Generation with LangChain 1.0+\n",
      "\n",
      "\n",
      "CORE CONCEPTS\n",
      "-------------\n",
      "\n",
      "1. Document Object Structure\n",
      "   - page_content: The actual text content\n",
      "   - metadata: Dictionary wit...\n",
      "\n",
      "üîç Metadata: {'source': './sample_data/notes.txt'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "# Load the notes.txt file\n",
    "txt_path = \"./sample_data/notes.txt\"\n",
    "\n",
    "if Path(txt_path).exists():\n",
    "    print(f\"Loading text file: {txt_path}\\n\")\n",
    "    \n",
    "    # Create loader\n",
    "    loader = TextLoader(txt_path, encoding=\"utf-8\")\n",
    "    \n",
    "    # Load the file\n",
    "    documents = loader.load()\n",
    "    \n",
    "    print(f\"‚úÖ Loaded {len(documents)} document\\n\")\n",
    "    \n",
    "    doc = documents[0]\n",
    "    print(f\"üìÑ Content length: {len(doc.page_content)} characters\")\n",
    "    print(f\"\\nüìù First 300 characters:\\n{doc.page_content[:300]}...\")\n",
    "    print(f\"\\nüîç Metadata: {doc.metadata}\")\n",
    "    \n",
    "else:\n",
    "    print(f\"‚ùå Text file not found: {txt_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markdown Files\n",
    "\n",
    "For Markdown files, use **UnstructuredMarkdownLoader** (preserves structure):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: unstructured[all] in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (0.18.21)\n",
      "Requirement already satisfied: charset-normalizer in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from unstructured[all]) (3.4.4)\n",
      "Requirement already satisfied: filetype in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from unstructured[all]) (1.2.0)\n",
      "Requirement already satisfied: python-magic in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from unstructured[all]) (0.4.27)\n",
      "Requirement already satisfied: lxml in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from unstructured[all]) (6.0.2)\n",
      "Requirement already satisfied: nltk in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from unstructured[all]) (3.9.2)\n",
      "Requirement already satisfied: requests in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from unstructured[all]) (2.32.5)\n",
      "Requirement already satisfied: beautifulsoup4 in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from unstructured[all]) (4.14.2)\n",
      "Requirement already satisfied: emoji in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from unstructured[all]) (2.15.0)\n",
      "Requirement already satisfied: dataclasses-json in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from unstructured[all]) (0.6.7)\n",
      "Requirement already satisfied: python-iso639 in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from unstructured[all]) (2025.11.16)\n",
      "Requirement already satisfied: langdetect in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from unstructured[all]) (1.0.9)\n",
      "Requirement already satisfied: numpy in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from unstructured[all]) (2.3.5)\n",
      "Requirement already satisfied: rapidfuzz in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from unstructured[all]) (3.14.3)\n",
      "Requirement already satisfied: backoff in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from unstructured[all]) (2.2.1)\n",
      "Requirement already satisfied: typing-extensions in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from unstructured[all]) (4.15.0)\n",
      "Requirement already satisfied: unstructured-client in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from unstructured[all]) (0.42.4)\n",
      "Requirement already satisfied: wrapt in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from unstructured[all]) (2.0.1)\n",
      "Requirement already satisfied: tqdm in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from unstructured[all]) (4.67.1)\n",
      "Requirement already satisfied: psutil in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from unstructured[all]) (7.1.3)\n",
      "Requirement already satisfied: python-oxmsg in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from unstructured[all]) (0.0.2)\n",
      "Requirement already satisfied: html5lib in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from unstructured[all]) (1.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from beautifulsoup4->unstructured[all]) (2.8)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from dataclasses-json->unstructured[all]) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from dataclasses-json->unstructured[all]) (0.9.0)\n",
      "Requirement already satisfied: packaging>=17.0 in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->unstructured[all]) (25.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json->unstructured[all]) (1.1.0)\n",
      "Requirement already satisfied: six>=1.9 in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from html5lib->unstructured[all]) (1.17.0)\n",
      "Requirement already satisfied: webencodings in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from html5lib->unstructured[all]) (0.5.1)\n",
      "Requirement already satisfied: click in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from nltk->unstructured[all]) (8.3.1)\n",
      "Requirement already satisfied: joblib in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from nltk->unstructured[all]) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from nltk->unstructured[all]) (2025.11.3)\n",
      "Requirement already satisfied: colorama in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from click->nltk->unstructured[all]) (0.4.6)\n",
      "Requirement already satisfied: olefile in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from python-oxmsg->unstructured[all]) (0.47)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from requests->unstructured[all]) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from requests->unstructured[all]) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from requests->unstructured[all]) (2025.11.12)\n",
      "Requirement already satisfied: aiofiles>=24.1.0 in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from unstructured-client->unstructured[all]) (25.1.0)\n",
      "Requirement already satisfied: cryptography>=3.1 in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from unstructured-client->unstructured[all]) (46.0.3)\n",
      "Requirement already satisfied: httpcore>=1.0.9 in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from unstructured-client->unstructured[all]) (1.0.9)\n",
      "Requirement already satisfied: httpx>=0.27.0 in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from unstructured-client->unstructured[all]) (0.28.1)\n",
      "Requirement already satisfied: pydantic>=2.11.2 in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from unstructured-client->unstructured[all]) (2.12.4)\n",
      "Requirement already satisfied: pypdf>=6.2.0 in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from unstructured-client->unstructured[all]) (6.3.0)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from unstructured-client->unstructured[all]) (1.0.0)\n",
      "Requirement already satisfied: cffi>=2.0.0 in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from cryptography>=3.1->unstructured-client->unstructured[all]) (2.0.0)\n",
      "Requirement already satisfied: pycparser in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from cffi>=2.0.0->cryptography>=3.1->unstructured-client->unstructured[all]) (2.23)\n",
      "Requirement already satisfied: h11>=0.16 in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from httpcore>=1.0.9->unstructured-client->unstructured[all]) (0.16.0)\n",
      "Requirement already satisfied: anyio in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from httpx>=0.27.0->unstructured-client->unstructured[all]) (4.11.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from pydantic>=2.11.2->unstructured-client->unstructured[all]) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from pydantic>=2.11.2->unstructured-client->unstructured[all]) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from pydantic>=2.11.2->unstructured-client->unstructured[all]) (0.4.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in d:\\aicode\\lcragbtcmp\\simple-rag-langchain\\lcrag_venv\\lib\\site-packages (from anyio->httpx>=0.27.0->unstructured-client->unstructured[all]) (1.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "‚ö†Ô∏è Unexpected error: No module named 'markdown'\n",
      "   Falling back to TextLoader.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: unstructured 0.18.21 does not provide the extra 'all'\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error loading README.md",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m     10\u001b[39m loader = UnstructuredMarkdownLoader(readme_path)\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m docs = \u001b[43mloader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m‚úÖ Loaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(docs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m document(s)\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AiCode\\LcRAGBtCmp\\simple-rag-langchain\\lcrag_venv\\Lib\\site-packages\\langchain_core\\document_loaders\\base.py:43\u001b[39m, in \u001b[36mBaseLoader.load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     38\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Load data into `Document` objects.\u001b[39;00m\n\u001b[32m     39\u001b[39m \n\u001b[32m     40\u001b[39m \u001b[33;03mReturns:\u001b[39;00m\n\u001b[32m     41\u001b[39m \u001b[33;03m    The documents.\u001b[39;00m\n\u001b[32m     42\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlazy_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AiCode\\LcRAGBtCmp\\simple-rag-langchain\\lcrag_venv\\Lib\\site-packages\\langchain_community\\document_loaders\\unstructured.py:107\u001b[39m, in \u001b[36mUnstructuredBaseLoader.lazy_load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    106\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Load file.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m107\u001b[39m elements = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_elements\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    108\u001b[39m \u001b[38;5;28mself\u001b[39m._post_process_elements(elements)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AiCode\\LcRAGBtCmp\\simple-rag-langchain\\lcrag_venv\\Lib\\site-packages\\langchain_community\\document_loaders\\markdown.py:94\u001b[39m, in \u001b[36mUnstructuredMarkdownLoader._get_elements\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_elements\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> List:\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01munstructured\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpartition\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmd\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m partition_md\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m partition_md(filename=\u001b[38;5;28mself\u001b[39m.file_path, **\u001b[38;5;28mself\u001b[39m.unstructured_kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AiCode\\LcRAGBtCmp\\simple-rag-langchain\\lcrag_venv\\Lib\\site-packages\\unstructured\\partition\\md.py:5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m IO, Any\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmarkdown\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrequests\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'markdown'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mUnicodeDecodeError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AiCode\\LcRAGBtCmp\\simple-rag-langchain\\lcrag_venv\\Lib\\site-packages\\langchain_community\\document_loaders\\text.py:43\u001b[39m, in \u001b[36mTextLoader.lazy_load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     42\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;28mself\u001b[39m.file_path, encoding=\u001b[38;5;28mself\u001b[39m.encoding) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m         text = \u001b[43mf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mUnicodeDecodeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\tools\\Anaconda3\\Lib\\encodings\\cp1252.py:23\u001b[39m, in \u001b[36mIncrementalDecoder.decode\u001b[39m\u001b[34m(self, input, final)\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, final=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcodecs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcharmap_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdecoding_table\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n",
      "\u001b[31mUnicodeDecodeError\u001b[39m: 'charmap' codec can't decode byte 0x81 in position 3042: character maps to <undefined>",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     22\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m   Falling back to TextLoader.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     23\u001b[39m         loader = TextLoader(readme_path)\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m         docs = \u001b[43mloader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   ‚úÖ Loaded with TextLoader: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(docs[\u001b[32m0\u001b[39m].page_content)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m chars\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AiCode\\LcRAGBtCmp\\simple-rag-langchain\\lcrag_venv\\Lib\\site-packages\\langchain_core\\document_loaders\\base.py:43\u001b[39m, in \u001b[36mBaseLoader.load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28mlist\u001b[39m[Document]:\n\u001b[32m     38\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Load data into `Document` objects.\u001b[39;00m\n\u001b[32m     39\u001b[39m \n\u001b[32m     40\u001b[39m \u001b[33;03m    Returns:\u001b[39;00m\n\u001b[32m     41\u001b[39m \u001b[33;03m        The documents.\u001b[39;00m\n\u001b[32m     42\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlazy_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AiCode\\LcRAGBtCmp\\simple-rag-langchain\\lcrag_venv\\Lib\\site-packages\\langchain_community\\document_loaders\\text.py:56\u001b[39m, in \u001b[36mTextLoader.lazy_load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     54\u001b[39m                 \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m     55\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.file_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     58\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.file_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: Error loading README.md"
     ]
    }
   ],
   "source": [
    "%pip install \"unstructured[all]\"\n",
    "\n",
    "# UnstructuredMarkdownLoader and TextLoader are already imported and available\n",
    "\n",
    "readme_path = \"README.md\"\n",
    "\n",
    "# Check if README.md exists\n",
    "if Path(readme_path).exists():\n",
    "    try:\n",
    "        loader = UnstructuredMarkdownLoader(readme_path)\n",
    "        docs = loader.load()\n",
    "        print(f\"‚úÖ Loaded {len(docs)} document(s)\")\n",
    "        print(f\"\\nFirst 200 chars:\\n{docs[0].page_content[:200]}...\")\n",
    "    except RuntimeError as e:\n",
    "        print(\"‚ö†Ô∏è UnstructuredMarkdownLoader failed with RuntimeError.\")\n",
    "        print(\"   Falling back to TextLoader.\")\n",
    "        loader = TextLoader(readme_path)\n",
    "        docs = loader.load()\n",
    "        print(f\"   ‚úÖ Loaded with TextLoader: {len(docs[0].page_content)} chars\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Unexpected error: {e}\")\n",
    "        print(\"   Falling back to TextLoader.\")\n",
    "        loader = TextLoader(readme_path)\n",
    "        docs = loader.load()\n",
    "        print(f\"   ‚úÖ Loaded with TextLoader: {len(docs[0].page_content)} chars\")\n",
    "else:\n",
    "    print(f\"‚ÑπÔ∏è No README.md found in current directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"batch-loading\"></a>\n",
    "## 8. Batch Loading with DirectoryLoader üìÇ\n",
    "\n",
    "### üî∞ BEGINNER\n",
    "\n",
    "**DirectoryLoader** loads all files from a directory automatically.\n",
    "\n",
    "Perfect for:\n",
    "- Loading entire document libraries\n",
    "- Processing multiple files at once\n",
    "- Building knowledge bases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loading all text files from: sample_data/\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 1482.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Loaded 1 text file(s)\n",
      "\n",
      "  - sample_data\\notes.txt (8639 chars)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "\n",
    "# Load all files from sample_data directory\n",
    "data_dir = \"sample_data\"\n",
    "\n",
    "if Path(data_dir).exists():\n",
    "    print(f\"üìÇ Loading all text files from: {data_dir}/\\n\")\n",
    "    \n",
    "    # Create loader for .txt files only\n",
    "    loader = DirectoryLoader(\n",
    "        data_dir,\n",
    "        glob=\"*.txt\",  # Pattern to match files\n",
    "        loader_cls=TextLoader,  # Use TextLoader for each file\n",
    "        show_progress=True  # Show progress bar\n",
    "    )\n",
    "    \n",
    "    # Load all matching files\n",
    "    documents = loader.load()\n",
    "    \n",
    "    print(f\"\\n‚úÖ Loaded {len(documents)} text file(s)\\n\")\n",
    "    \n",
    "    # Show sources\n",
    "    for doc in documents:\n",
    "        print(f\"  - {doc.metadata['source']} ({len(doc.page_content)} chars)\")\n",
    "        \n",
    "else:\n",
    "    print(f\"‚ùå Directory not found: {data_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéì INTERMEDIATE: Loading Multiple File Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loading from: .\\sample_data\n",
      "\n",
      "  ‚úÖ TXT: notes.txt\n",
      "  ‚úÖ CSV: products.csv (15 rows)\n",
      "  ‚úÖ JSON: api_response.json\n",
      "\n",
      "üìä Total: 17 documents loaded\n",
      "\n",
      "üìà Summary:\n",
      "   Files loaded: 3\n",
      "   Total documents: 17\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import CSVLoader\n",
    "\n",
    "# Advanced: Load all files from a directory (mixed types)\n",
    "# This function handles different file types intelligently\n",
    "\n",
    "def load_all_documents(directory: str) -> list:\n",
    "    \"\"\"\n",
    "    Load documents from multiple file formats in a directory.\n",
    "    \n",
    "    Supports: PDF, TXT, CSV, JSON, HTML\n",
    "    \"\"\"\n",
    "    all_docs = []\n",
    "    directory_path = Path(directory)\n",
    "    \n",
    "    if not directory_path.exists():\n",
    "        print(f\"‚ùå Directory not found: {directory}\")\n",
    "        return []\n",
    "    \n",
    "    print(f\"üìÇ Loading from: {directory}\\n\")\n",
    "    \n",
    "    # Load PDFs\n",
    "    pdf_files = list(directory_path.glob(\"*.pdf\"))\n",
    "    for pdf in pdf_files:\n",
    "        loader = PyPDFLoader(str(pdf))\n",
    "        docs = loader.load()\n",
    "        all_docs.extend(docs)\n",
    "        print(f\"  ‚úÖ PDF: {pdf.name} ({len(docs)} pages)\")\n",
    "    \n",
    "    # Load TXT files\n",
    "    txt_files = list(directory_path.glob(\"*.txt\"))\n",
    "    for txt in txt_files:\n",
    "        loader = TextLoader(str(txt))\n",
    "        docs = loader.load()\n",
    "        all_docs.extend(docs)\n",
    "        print(f\"  ‚úÖ TXT: {txt.name}\")\n",
    "    \n",
    "    # Load CSV files\n",
    "    csv_files = list(directory_path.glob(\"*.csv\"))\n",
    "    for csv in csv_files:\n",
    "        loader = CSVLoader(str(csv))\n",
    "        docs = loader.load()\n",
    "        all_docs.extend(docs)\n",
    "        print(f\"  ‚úÖ CSV: {csv.name} ({len(docs)} rows)\")\n",
    "    \n",
    "    # Load JSON files\n",
    "    json_files = list(directory_path.glob(\"*.json\"))\n",
    "    for json_file in json_files:\n",
    "        try:\n",
    "            loader = JSONLoader(\n",
    "                str(json_file),\n",
    "                jq_schema=\".\",\n",
    "                text_content=False\n",
    "            )\n",
    "            docs = loader.load()\n",
    "            all_docs.extend(docs)\n",
    "            print(f\"  ‚úÖ JSON: {json_file.name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ö†Ô∏è JSON: {json_file.name} (error: {str(e)[:50]}...)\")\n",
    "    \n",
    "    print(f\"\\nüìä Total: {len(all_docs)} documents loaded\")\n",
    "    return all_docs\n",
    "\n",
    "# Test the function\n",
    "if Path(\"sample_data\").exists():\n",
    "    all_documents = load_all_documents(\".\\\\sample_data\")\n",
    "    \n",
    "    # Show summary\n",
    "    print(f\"\\nüìà Summary:\")\n",
    "    sources = [doc.metadata['source'] for doc in all_documents]\n",
    "    print(f\"   Files loaded: {len(set(sources))}\")\n",
    "    print(f\"   Total documents: {len(all_documents)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"comparison\"></a>\n",
    "## 9. Loader Comparison Table üìä\n",
    "\n",
    "### üî∞ BEGINNER REFERENCE\n",
    "\n",
    "| Loader | File Type | Use Case | Documents Created |\n",
    "|--------|-----------|----------|-------------------|\n",
    "| **PyPDFLoader** | `.pdf` | Research papers, books, reports | 1 per page |\n",
    "| **CSVLoader** | `.csv` | Product catalogs, data tables | 1 per row |\n",
    "| **JSONLoader** | `.json` | API responses, config files | Depends on jq query |\n",
    "| **WebBaseLoader** | Web URLs | Blog posts, documentation | 1 per URL |\n",
    "| **TextLoader** | `.txt` | Plain text, logs | 1 per file |\n",
    "| **UnstructuredMarkdownLoader** | `.md` | Documentation, notes | 1 per file |\n",
    "| **DirectoryLoader** | Multiple | Batch processing | All files matching pattern |\n",
    "\n",
    "### When to Use Which?\n",
    "\n",
    "- üìï **Academic papers?** ‚Üí PyPDFLoader\n",
    "- üìä **Structured data?** ‚Üí CSVLoader\n",
    "- üîß **API data?** ‚Üí JSONLoader\n",
    "- üåê **Web content?** ‚Üí WebBaseLoader\n",
    "- üìù **Simple text?** ‚Üí TextLoader\n",
    "- üìÇ **Entire folder?** ‚Üí DirectoryLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"best-practices\"></a>\n",
    "## 10. Best Practices üåü\n",
    "\n",
    "### üî∞ BEGINNER TIPS\n",
    "\n",
    "#### 1. Always Check File Existence\n",
    "```python\n",
    "# ‚úÖ Good\n",
    "if Path(file_path).exists():\n",
    "    loader = PyPDFLoader(file_path)\n",
    "    docs = loader.load()\n",
    "else:\n",
    "    print(f\"File not found: {file_path}\")\n",
    "\n",
    "# ‚ùå Bad - Will crash if file doesn't exist\n",
    "loader = PyPDFLoader(file_path)\n",
    "docs = loader.load()\n",
    "```\n",
    "\n",
    "#### 2. Use Lazy Loading for Large Files\n",
    "```python\n",
    "# For PDFs > 100 pages or files > 10MB\n",
    "for page in loader.lazy_load():\n",
    "    process_page(page)\n",
    "```\n",
    "\n",
    "#### 3. Inspect Metadata\n",
    "```python\n",
    "# Always check what metadata is available\n",
    "print(docs[0].metadata)\n",
    "```\n",
    "\n",
    "### üéì INTERMEDIATE TIPS\n",
    "\n",
    "#### 1. Error Handling\n",
    "```python\n",
    "try:\n",
    "    loader = PyPDFLoader(pdf_path)\n",
    "    docs = loader.load()\n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found: {pdf_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading {pdf_path}: {e}\")\n",
    "```\n",
    "\n",
    "#### 2. Add Custom Metadata\n",
    "```python\n",
    "# Add custom metadata after loading\n",
    "for doc in documents:\n",
    "    doc.metadata['loaded_at'] = datetime.now().isoformat()\n",
    "    doc.metadata['category'] = 'research_paper'\n",
    "```\n",
    "\n",
    "#### 3. Filter Documents\n",
    "```python\n",
    "# Filter by metadata\n",
    "research_docs = [\n",
    "    doc for doc in all_documents \n",
    "    if 'research' in doc.metadata['source'].lower()\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"summary\"></a>\n",
    "## 11. Summary & Exercises üìù\n",
    "\n",
    "### üéâ What You Learned\n",
    "\n",
    "‚úÖ **Document Loaders** convert files into standardized Document objects\n",
    "\n",
    "‚úÖ **PyPDFLoader** loads PDF files (1 document per page)\n",
    "\n",
    "‚úÖ **CSVLoader** loads CSV data (1 document per row)\n",
    "\n",
    "‚úÖ **JSONLoader** uses jq syntax to extract data from JSON\n",
    "\n",
    "‚úÖ **WebBaseLoader** scrapes web pages (static HTML only)\n",
    "\n",
    "‚úÖ **TextLoader** handles plain text files\n",
    "\n",
    "‚úÖ **DirectoryLoader** batch processes multiple files\n",
    "\n",
    "‚úÖ All loaders return **Document** objects with `page_content` and `metadata`\n",
    "\n",
    "### üí° Practice Exercises\n",
    "\n",
    "#### üî∞ Beginner Exercises\n",
    "\n",
    "1. **Load a PDF and count pages**\n",
    "   - Use PyPDFLoader to load `attention.pdf`\n",
    "   - Print the number of pages\n",
    "   - Print the first 100 characters of page 1\n",
    "\n",
    "2. **Load CSV and find products by category**\n",
    "   - Load `products.csv`\n",
    "   - Filter documents to find only \"Electronics\"\n",
    "   - Print product names\n",
    "\n",
    "3. **Combine multiple files**\n",
    "   - Load notes.txt, products.csv, and api_response.json\n",
    "   - Count total documents\n",
    "   - Print unique sources\n",
    "\n",
    "#### üéì Intermediate Exercises\n",
    "\n",
    "1. **Build a multi-format loader**\n",
    "   - Create a function that accepts a directory path\n",
    "   - Automatically detect file types (.pdf, .csv, .json, .txt)\n",
    "   - Load all files and add custom metadata (file_type, loaded_date)\n",
    "\n",
    "2. **Extract specific data from JSON**\n",
    "   - Load `api_response.json`\n",
    "   - Use jq to extract only article titles\n",
    "   - Create a summary document with all titles\n",
    "\n",
    "3. **Lazy load and process**\n",
    "   - Use lazy_load() on a PDF\n",
    "   - Process each page and extract pages containing specific keywords\n",
    "   - Save filtered pages to a new list\n",
    "\n",
    "### üìö Next Steps\n",
    "\n",
    "In **Notebook 03: Text Splitting Strategies**, you'll learn how to:\n",
    "- Split long documents into chunks\n",
    "- Choose optimal chunk sizes\n",
    "- Handle overlap for better context\n",
    "- Use different splitters for different content types\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations! You now know how to load data from any source! üéâ**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lcrag_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
